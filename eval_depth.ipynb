{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d053efcb-ef73-4aee-ba09-650482218971",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T14:06:14.417676Z",
     "iopub.status.busy": "2024-08-28T14:06:14.416801Z",
     "iopub.status.idle": "2024-08-28T14:06:20.719928Z",
     "shell.execute_reply": "2024-08-28T14:06:20.718873Z",
     "shell.execute_reply.started": "2024-08-28T14:06:14.417613Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-28 22:06:16.896715: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-28 22:06:16.914724: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-28 22:06:16.936569: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-28 22:06:16.943145: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-28 22:06:16.960081: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2024 The Google Research Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import pipeline\n",
    "import matplotlib\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "\n",
    "import pdb\n",
    "\n",
    "\n",
    "def filter_paths(paths, filter_file=None):\n",
    "    if filter_file is not None:\n",
    "        filter_names = set(json.load(open(filter_file)))\n",
    "        paths = [image for image in paths if os.path.basename(image) in filter_names]\n",
    "    return paths\n",
    "\n",
    "def make_folder(folder):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "def get_depth_tensor(pipe, image):\n",
    "    depth_tensor = pipe(image)[\"predicted_depth\"]\n",
    "    depth_tensor = torch.nn.functional.interpolate(\n",
    "        depth_tensor.unsqueeze(1),\n",
    "        size=image.size[::-1],\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    )[0]\n",
    "    return depth_tensor\n",
    "\n",
    "# def main(args):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e06e5199-6afc-4ec8-9847-b9e19f45a210",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T13:16:00.248797Z",
     "iopub.status.busy": "2024-08-28T13:16:00.247946Z",
     "iopub.status.idle": "2024-08-28T13:16:05.012208Z",
     "shell.execute_reply": "2024-08-28T13:16:05.009987Z",
     "shell.execute_reply.started": "2024-08-28T13:16:00.248734Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms.v2 as transforms\n",
    "from utils.dataset import process_frames\n",
    "height, width = 512, 512\n",
    "# dataset_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/open-images/val/hed/val_dataset.json\"\n",
    "# dataset_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/open-images/Human_body/openpose/val_dataset.json\"\n",
    "dataset_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/PascalVOC/VOC2012/depth_anything_v2_gray/val_dataset.json\"\n",
    "# dataset_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/PascalVOC/VOC2012/hed/val_dataset.json\"\n",
    "val_dataset = json.load(open(dataset_path))\n",
    "image_paths = [line['original_image'] for line in val_dataset]\n",
    "real_images = [Image.open(path).convert(\"RGB\") for path in image_paths]\n",
    "\n",
    "real_image_path = os.path.dirname(dataset_path)\n",
    "generate_image_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/output_final/output_depth_lora_joint_gray_rank64_nta_convfuse/eval\"\n",
    "generate_image_paths = [os.path.join(generate_image_path, line['file_name']) for line in val_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ac0cc66-7316-461b-a948-6e36d4dbd8b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T12:31:12.039972Z",
     "iopub.status.busy": "2024-08-28T12:31:12.039092Z",
     "iopub.status.idle": "2024-08-28T12:31:26.066124Z",
     "shell.execute_reply": "2024-08-28T12:31:26.065084Z",
     "shell.execute_reply.started": "2024-08-28T12:31:12.039878Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    image = process_frames([image], height, width,  verbose = False, div = 8, rand_crop=False)[0]\n",
    "    # image = ToTensor(image)\n",
    "    # return F.center_crop(image, (256, 256))\n",
    "    return image\n",
    "\n",
    "processed_real_images = [preprocess_image(image) for image in real_images]\n",
    "\n",
    "# torch.Size([10, 3, 256, 256])\n",
    "\n",
    "fake_images = [Image.open(path).convert(\"RGB\") for path in generate_image_paths]\n",
    "processed_fake_images = [preprocess_image(image) for image in fake_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdb6e3e7-3443-40ac-b76b-9ab03ebbffc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T12:32:28.532710Z",
     "iopub.status.busy": "2024-08-28T12:32:28.532032Z",
     "iopub.status.idle": "2024-08-28T12:32:28.541721Z",
     "shell.execute_reply": "2024-08-28T12:32:28.539734Z",
     "shell.execute_reply.started": "2024-08-28T12:32:28.532649Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"http_proxy\"]=\"http://127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"]=\"http://127.0.0.1:7890\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5599bb3f-b4c3-4686-8cf9-bce36f74ae78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T12:32:30.875738Z",
     "iopub.status.busy": "2024-08-28T12:32:30.873241Z",
     "iopub.status.idle": "2024-08-28T12:32:35.838230Z",
     "shell.execute_reply": "2024-08-28T12:32:35.837091Z",
     "shell.execute_reply.started": "2024-08-28T12:32:30.875665Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a97f7a07aae4e458d965ab2c1ff2a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/950 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6a110adf6847db9529b6c38c03f3a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/99.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31379a28fc834536b8ef19acba23a37c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/775 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Init pipeline\n",
    "pipe = pipeline(task=\"depth-estimation\", model=\"depth-anything/Depth-Anything-V2-Small-hf\")\n",
    "cmap = matplotlib.colormaps.get_cmap('Spectral_r')\n",
    "\n",
    "real_depth_tensors = []\n",
    "fake_depth_tensors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93791c00-07c1-459f-b1f8-d234a04c1a4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T12:32:44.804263Z",
     "iopub.status.busy": "2024-08-28T12:32:44.803393Z",
     "iopub.status.idle": "2024-08-28T12:32:44.812880Z",
     "shell.execute_reply": "2024-08-28T12:32:44.811495Z",
     "shell.execute_reply.started": "2024-08-28T12:32:44.804200Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def depth_tensor_to_image(depth_tensor):\n",
    "    depth_tensor = (depth_tensor - depth_tensor.min()) / (depth_tensor.max() - depth_tensor.min()) * 255.0\n",
    "    depth_tensor = depth_tensor.to(torch.uint8)\n",
    "    gray_depth = T.ToPILImage()(depth_tensor)\n",
    "    return gray_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76a4fd87-8ce6-4570-a014-7c1919077de3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T12:38:02.264068Z",
     "iopub.status.busy": "2024-08-28T12:38:02.263196Z",
     "iopub.status.idle": "2024-08-28T12:45:55.561067Z",
     "shell.execute_reply": "2024-08-28T12:45:55.558767Z",
     "shell.execute_reply.started": "2024-08-28T12:38:02.264004Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 594/594 [07:49<00:00,  1.26it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for real_image, fake_image in zip(tqdm(processed_real_images[len(real_depth_tensors):]), processed_fake_images[len(real_depth_tensors):]):\n",
    "    # Predict pseudo label\n",
    "    real_depth_tensor = get_depth_tensor(pipe, real_image)\n",
    "    fake_depth_tensor = get_depth_tensor(pipe, fake_image)\n",
    "    real_depth_tensors.append(real_depth_tensor)\n",
    "    fake_depth_tensors.append(fake_depth_tensor)\n",
    "\n",
    "real_depth_tensors = torch.stack(real_depth_tensors)\n",
    "fake_depth_tensors = torch.stack(fake_depth_tensors)\n",
    "\n",
    "torch.save(real_depth_tensors, os.path.join(real_image_path, \"depth_tensor.pt\"))\n",
    "torch.save(fake_depth_tensors, os.path.join(generate_image_path, \"depth_tensor.pt\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # conda run -n control python3 annotate_spatial.py --config_path configs/annotate_spatial.yaml\n",
    "# \tparser = argparse.ArgumentParser(description=\"\")\n",
    "# \tparser.add_argument(\"--config_path\", type=str, default = None)\n",
    "# \targs = parser.parse_args()\n",
    "# \tmain(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0ef1567-916c-4b2d-ad48-8a207aad2399",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T14:06:20.730627Z",
     "iopub.status.busy": "2024-08-28T14:06:20.730389Z",
     "iopub.status.idle": "2024-08-28T14:06:33.864758Z",
     "shell.execute_reply": "2024-08-28T14:06:33.863237Z",
     "shell.execute_reply.started": "2024-08-28T14:06:20.730603Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms.v2 as transforms\n",
    "from utils.dataset import process_frames\n",
    "height, width = 512, 512\n",
    "# dataset_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/open-images/val/hed/val_dataset.json\"\n",
    "# dataset_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/open-images/Human_body/openpose/val_dataset.json\"\n",
    "dataset_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/PascalVOC/VOC2012/depth_anything_v2_gray/val_dataset.json\"\n",
    "# dataset_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/PascalVOC/VOC2012/hed/val_dataset.json\"\n",
    "val_dataset = json.load(open(dataset_path))\n",
    "\n",
    "real_image_path = os.path.dirname(dataset_path)\n",
    "generate_image_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/readout_guidance/results/spatial/datv2_gray\"\n",
    "generate_image_paths = [os.path.join(generate_image_path, line['file_name']) for line in val_dataset]\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = process_frames([image], height, width,  verbose = False, div = 8, rand_crop=False)[0]\n",
    "    # image = ToTensor(image)\n",
    "    # return F.center_crop(image, (256, 256))\n",
    "    return image\n",
    "\n",
    "# torch.Size([10, 3, 256, 256])\n",
    "\n",
    "fake_images = [Image.open(path).convert(\"RGB\") for path in generate_image_paths]\n",
    "processed_fake_images = [preprocess_image(image) for image in fake_images]\n",
    "\n",
    "\n",
    "pipe = pipeline(task=\"depth-estimation\", model=\"depth-anything/Depth-Anything-V2-Small-hf\")\n",
    "cmap = matplotlib.colormaps.get_cmap('Spectral_r')\n",
    "fake_depth_tensors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f9a4ae8-bc6a-4f20-a5f3-92ebf0c15691",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T14:06:33.905964Z",
     "iopub.status.busy": "2024-08-28T14:06:33.905717Z",
     "iopub.status.idle": "2024-08-28T14:13:13.994736Z",
     "shell.execute_reply": "2024-08-28T14:13:13.993939Z",
     "shell.execute_reply.started": "2024-08-28T14:06:33.905940Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [06:39<00:00,  2.50it/s]\n"
     ]
    }
   ],
   "source": [
    "for fake_image in tqdm(processed_fake_images[len(fake_depth_tensors):]):\n",
    "    # Predict pseudo label\n",
    "    fake_depth_tensor = get_depth_tensor(pipe, fake_image)\n",
    "    fake_depth_tensors.append(fake_depth_tensor)\n",
    "\n",
    "fake_depth_tensors = torch.stack(fake_depth_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5857df16-7f2a-4b21-870b-b652f4ca4908",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T14:13:34.794283Z",
     "iopub.status.busy": "2024-08-28T14:13:34.793376Z",
     "iopub.status.idle": "2024-08-28T14:13:34.804023Z",
     "shell.execute_reply": "2024-08-28T14:13:34.802618Z",
     "shell.execute_reply.started": "2024-08-28T14:13:34.794218Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1, 512, 512])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_depth_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5b94978-4543-40fc-8796-c2e7eb117b0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T14:13:35.597695Z",
     "iopub.status.busy": "2024-08-28T14:13:35.596823Z",
     "iopub.status.idle": "2024-08-28T14:13:37.297546Z",
     "shell.execute_reply": "2024-08-28T14:13:37.295984Z",
     "shell.execute_reply.started": "2024-08-28T14:13:35.597629Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# torch.save(real_depth_tensors, os.path.join(real_image_path, \"depth_tensor.pt\"))\n",
    "torch.save(fake_depth_tensors, os.path.join(generate_image_path, \"depth_tensor.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d039343-0859-4291-a26e-3980cc886aa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T14:13:46.271052Z",
     "iopub.status.busy": "2024-08-28T14:13:46.270648Z",
     "iopub.status.idle": "2024-08-28T14:13:49.256441Z",
     "shell.execute_reply": "2024-08-28T14:13:49.254700Z",
     "shell.execute_reply.started": "2024-08-28T14:13:46.271018Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "real_depth_tensors = torch.load(os.path.join(real_image_path, \"depth_tensor.pt\"))\n",
    "fake_depth_tensors = torch.load(os.path.join(generate_image_path, \"depth_tensor.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5af98e35-b8a6-4931-8bf3-1acff6023595",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T14:14:03.050002Z",
     "iopub.status.busy": "2024-08-28T14:14:03.049067Z",
     "iopub.status.idle": "2024-08-28T14:14:03.070187Z",
     "shell.execute_reply": "2024-08-28T14:14:03.068955Z",
     "shell.execute_reply.started": "2024-08-28T14:14:03.049867Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append(\"data/deps/ControlNet\")\n",
    "sys.path.append(\"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/Marigold\")\n",
    "from tabulate import tabulate\n",
    "from src.util import metric\n",
    "from src.util.alignment import (\n",
    "    align_depth_least_square,\n",
    "    depth2disparity,\n",
    "    disparity2depth,\n",
    ")\n",
    "\n",
    "from src.util.metric import MetricTracker\n",
    "\n",
    "eval_metrics = [\n",
    "    \"abs_relative_difference\",\n",
    "    \"squared_relative_difference\",\n",
    "    \"rmse_linear\",\n",
    "    \"rmse_log\",\n",
    "    \"log10\",\n",
    "    \"delta1_acc\",\n",
    "    \"delta2_acc\",\n",
    "    \"delta3_acc\",\n",
    "    \"i_rmse\",\n",
    "    \"silog_rmse\",\n",
    "]\n",
    "\n",
    "metric_funcs = [getattr(metric, _met) for _met in eval_metrics]\n",
    "\n",
    "metric_tracker = MetricTracker(*[m.__name__ for m in metric_funcs])\n",
    "metric_tracker.reset()\n",
    "\n",
    "output_dir = generate_image_path\n",
    "\n",
    "per_sample_filename = os.path.join(output_dir, \"per_sample_metrics.csv\")\n",
    "# write title\n",
    "file_names = [item[\"file_name\"].split(\".\")[0] for item in val_dataset]\n",
    "\n",
    "with open(per_sample_filename, \"w+\") as f:\n",
    "    f.write(\"filename,\")\n",
    "    f.write(\",\".join([m.__name__ for m in metric_funcs]))\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab1d201a-1187-47dc-bd83-136f0b195015",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T14:14:37.632756Z",
     "iopub.status.busy": "2024-08-28T14:14:37.631891Z",
     "iopub.status.idle": "2024-08-28T14:14:59.767127Z",
     "shell.execute_reply": "2024-08-28T14:14:59.766596Z",
     "shell.execute_reply.started": "2024-08-28T14:14:37.632694Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:22<00:00, 45.22it/s]\n"
     ]
    }
   ],
   "source": [
    "for real_depth_tensor, fake_depth_tensor, pred_name in zip(tqdm(real_depth_tensors), fake_depth_tensors, file_names):\n",
    "    depth_raw = real_depth_tensor.clone().numpy()\n",
    "    depth_pred = fake_depth_tensor.clone().numpy()\n",
    "    depth_raw += 1\n",
    "    valid_mask = np.ones_like(depth_raw).astype(bool)\n",
    "    depth_pred, scale, shift = align_depth_least_square(\n",
    "        gt_arr=depth_raw,\n",
    "        pred_arr=depth_pred,\n",
    "        valid_mask_arr=valid_mask,\n",
    "        return_scale_shift=True,\n",
    "        # max_resolution=alignment_max_res,\n",
    "    )\n",
    "\n",
    "    # clip to d > 0 for evaluation\n",
    "    depth_pred = np.clip(depth_pred, a_min=1e-6, a_max=None)\n",
    "\n",
    "    # Evaluate (using CUDA if available)\n",
    "    sample_metric = []\n",
    "    depth_pred_ts = torch.from_numpy(depth_pred).to(device)\n",
    "    depth_raw_ts = torch.from_numpy(depth_raw).to(device)\n",
    "    valid_mask_ts = torch.from_numpy(valid_mask).to(device)\n",
    "\n",
    "    for met_func in metric_funcs:\n",
    "        _metric_name = met_func.__name__\n",
    "        _metric = met_func(depth_pred_ts, depth_raw_ts, valid_mask_ts).item()\n",
    "        sample_metric.append(_metric.__str__())\n",
    "        metric_tracker.update(_metric_name, _metric)\n",
    "\n",
    "    # Save per-sample metric\n",
    "    with open(per_sample_filename, \"a+\") as f:\n",
    "        f.write(pred_name + \",\")\n",
    "        f.write(\",\".join(sample_metric))\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "663524d6-24b3-4ec9-947f-47e0a6494aae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T14:14:59.825785Z",
     "iopub.status.busy": "2024-08-28T14:14:59.825589Z",
     "iopub.status.idle": "2024-08-28T14:14:59.834480Z",
     "shell.execute_reply": "2024-08-28T14:14:59.833615Z",
     "shell.execute_reply.started": "2024-08-28T14:14:59.825765Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics saved to /home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/readout_guidance/results/spatial/datv2_gray/eval_metrics.txt\n"
     ]
    }
   ],
   "source": [
    "# -------------------- Save metrics to file --------------------\n",
    "eval_text = f\"Evaluation metrics:\\n\\\n",
    "of predictions: {generate_image_path}\\n\\\n",
    "on dataset: {real_image_path}\\n\"\n",
    "\n",
    "eval_text += tabulate(\n",
    "    [metric_tracker.result().keys(), metric_tracker.result().values()]\n",
    ")\n",
    "\n",
    "metrics_filename = \"eval_metrics\"\n",
    "metrics_filename += \".txt\"\n",
    "\n",
    "_save_to = os.path.join(output_dir, metrics_filename)\n",
    "with open(_save_to, \"w+\") as f:\n",
    "    f.write(eval_text)\n",
    "    print(f\"Evaluation metrics saved to {_save_to}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c69622bf-a024-48c1-8ef4-e173229c4db3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T13:17:09.694672Z",
     "iopub.status.busy": "2024-08-28T13:17:09.693725Z",
     "iopub.status.idle": "2024-08-28T13:17:09.757259Z",
     "shell.execute_reply": "2024-08-28T13:17:09.756374Z",
     "shell.execute_reply.started": "2024-08-28T13:17:09.694608Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_name = val_dataset[0][\"file_name\"].split(\".\")[0]\n",
    "depth_raw = real_depth_tensors[0].clone().numpy()\n",
    "depth_raw += 1\n",
    "depth_pred = fake_depth_tensors[0].clone().numpy()\n",
    "valid_mask = np.ones_like(depth_raw).astype(bool)\n",
    "depth_pred, scale, shift = align_depth_least_square(\n",
    "    gt_arr=depth_raw,\n",
    "    pred_arr=depth_pred,\n",
    "    valid_mask_arr=valid_mask,\n",
    "    return_scale_shift=True,\n",
    "    # max_resolution=alignment_max_res,\n",
    ")\n",
    "\n",
    "depth_pred = np.clip(depth_pred, a_min=1, a_max=None)\n",
    "\n",
    "# Evaluate (using CUDA if available)\n",
    "sample_metric = []\n",
    "device = \"cuda\"\n",
    "depth_pred_ts = torch.from_numpy(depth_pred).to(device)\n",
    "depth_raw_ts = torch.from_numpy(depth_raw).to(device)\n",
    "valid_mask_ts = torch.from_numpy(valid_mask).to(device)\n",
    "for met_func in metric_funcs:\n",
    "    _metric_name = met_func.__name__\n",
    "    _metric = met_func(depth_pred_ts, depth_raw_ts, valid_mask_ts).item()\n",
    "    sample_metric.append(_metric.__str__())\n",
    "    metric_tracker.update(_metric_name, _metric)\n",
    "\n",
    "# Save per-sample metric\n",
    "with open(per_sample_filename, \"a+\") as f:\n",
    "    f.write(pred_name + \",\")\n",
    "    f.write(\",\".join(sample_metric))\n",
    "    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21699376-81d0-4e95-9b52-1b1fbe6aa251",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T13:52:40.521893Z",
     "iopub.status.busy": "2024-08-31T13:52:40.521053Z",
     "iopub.status.idle": "2024-08-31T13:52:40.767052Z",
     "shell.execute_reply": "2024-08-31T13:52:40.766026Z",
     "shell.execute_reply.started": "2024-08-31T13:52:40.521831Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "a = json.load(open(\"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/MSRVTT/MSR_VTT.json\"))\n",
    "\n",
    "dataset_dict = dict()\n",
    "for ann in a['annotations']:\n",
    "    if ann[\"image_id\"] not in dataset_dict:\n",
    "        dataset_dict[ann[\"image_id\"]] = []\n",
    "    dataset_dict[ann[\"image_id\"]].append(ann[\"caption\"])\n",
    "\n",
    "dataset_list = []\n",
    "dataset_key_list = sorted(list(dataset_dict.keys()), key=lambda x: int(x.strip(\"video\")))\n",
    "\n",
    "for data in dataset_key_list:\n",
    "    video_id = int(data.strip(\"video\"))\n",
    "    dataset_list.append({\"video_id\": video_id, \"text\": dataset_dict[data]})\n",
    "\n",
    "\n",
    "with open(\"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/MSRVTT/captions.json\", \"w\") as f:\n",
    "    json.dump(dataset_dict, f)\n",
    "\n",
    "from utils.dataset import MSRVTT\n",
    "\n",
    "video_folder = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/MSRVTT/videos\"\n",
    "caption_file = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/MSRVTT/captions.json\"\n",
    "dataset = MSRVTT(video_folder, caption_file,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bde4258-50eb-4024-a83f-663a4b24698b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T12:09:30.345879Z",
     "iopub.status.busy": "2024-09-01T12:09:30.345482Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/CelebA/dataset_celeba_wild_idpairs_fix.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c89327e-1327-42be-be28-cb0f73a55345",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T12:04:03.575098Z",
     "iopub.status.busy": "2024-09-01T12:04:03.542809Z",
     "iopub.status.idle": "2024-09-01T12:04:03.618394Z",
     "shell.execute_reply": "2024-09-01T12:04:03.614427Z",
     "shell.execute_reply.started": "2024-09-01T12:04:03.575033Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image0': '/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/CelebA/img_celeba/119614.jpg',\n",
       " 'image1': '/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/CelebA/img_celeba/015280.jpg',\n",
       " 'person_id': 5195,\n",
       " 'text0': 'arafed woman in a purple dress and a tiara smiling',\n",
       " 'text1': 'smiling woman with long brown hair and white shirt posing for a picture'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0bb5c17-98a3-4915-b367-961e8f639e97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-31T18:14:01.719326Z",
     "iopub.status.busy": "2024-08-31T18:14:01.718489Z",
     "iopub.status.idle": "2024-08-31T18:14:01.754475Z",
     "shell.execute_reply": "2024-08-31T18:14:01.753524Z",
     "shell.execute_reply.started": "2024-08-31T18:14:01.719264Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The arguments passed translate to:\n",
      "\n",
      "{'password': '', 'username': 'oauth2'}\n",
      "\n",
      "Combining these with the CLI defaults gives:\n",
      "\n",
      "{'extract_flat': 'discard_in_playlist',\n",
      " 'fragment_retries': 10,\n",
      " 'ignoreerrors': 'only_download',\n",
      " 'password': '',\n",
      " 'postprocessors': [{'key': 'FFmpegConcat',\n",
      "                     'only_multi_video': True,\n",
      "                     'when': 'playlist'}],\n",
      " 'retries': 10,\n",
      " 'username': 'oauth2'}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# Allow direct execution\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "\n",
    "import yt_dlp\n",
    "import yt_dlp.options\n",
    "\n",
    "create_parser = yt_dlp.options.create_parser\n",
    "\n",
    "def parse_patched_options(opts):\n",
    "    patched_parser = create_parser()\n",
    "    patched_parser.defaults.update({\n",
    "        'ignoreerrors': False,\n",
    "        'retries': 0,\n",
    "        'fragment_retries': 0,\n",
    "        'extract_flat': False,\n",
    "        'concat_playlist': 'never',\n",
    "    })\n",
    "    yt_dlp.options.create_parser = lambda: patched_parser\n",
    "    try:\n",
    "        return yt_dlp.parse_options(opts)\n",
    "    finally:\n",
    "        yt_dlp.options.create_parser = create_parser\n",
    "\n",
    "\n",
    "default_opts = parse_patched_options([]).ydl_opts\n",
    "\n",
    "\n",
    "def cli_to_api(opts, cli_defaults=False):\n",
    "    opts = (yt_dlp.parse_options if cli_defaults else parse_patched_options)(opts).ydl_opts\n",
    "\n",
    "    diff = {k: v for k, v in opts.items() if default_opts[k] != v}\n",
    "    if 'postprocessors' in diff:\n",
    "        diff['postprocessors'] = [pp for pp in diff['postprocessors']\n",
    "                                  if pp not in default_opts['postprocessors']]\n",
    "    return diff\n",
    "\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "# --username oauth2 --password ''\n",
    "#  --extractor-args \"youtube:player_client=default,-web_creator\"\n",
    "args = [\"--username\", \"oauth2\" ,\"--password\", \"\"]\n",
    "print('\\nThe arguments passed translate to:\\n')\n",
    "pprint(cli_to_api(args))\n",
    "print('\\nCombining these with the CLI defaults gives:\\n')\n",
    "pprint(cli_to_api(args, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ba36c1d-6cca-4cb2-915d-361f29b5e836",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T10:31:45.937233Z",
     "iopub.status.busy": "2024-09-02T10:31:45.936495Z",
     "iopub.status.idle": "2024-09-02T10:31:45.952409Z",
     "shell.execute_reply": "2024-09-02T10:31:45.950122Z",
     "shell.execute_reply.started": "2024-09-02T10:31:45.937170Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-  -\\na  a\\nb  b\\nc  c\\n-  -'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tabulate import tabulate\r\n",
    "metrics = {\"a\":\"a\", \"b\":\"b\", \"c\":\"c\"}\r\n",
    "\r\n",
    "tabulate(zip(metrics.keys(), metrics.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934861dd-9685-4066-a830-2486261dd68a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jointdiff)",
   "language": "python",
   "name": "jointdiff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
