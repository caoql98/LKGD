{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a265677-21b4-4913-9c84-e5cf82e1b35c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HF_HOME\"] = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/.cache/huggingface\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e060cd4e-b2b3-4ac7-9521-4d312fc50698",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from patch import patch\n",
    "\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, DiffusionPipeline, StableDiffusionPipeline, UNet2DConditionModel\n",
    "from diffusers.loaders import UNet2DConditionLoadersMixin, PeftAdapterMixin, FromSingleFileMixin\n",
    "from pipeline.pipeline_stable_diffusion_joint_control import StableDiffusionPipelineJointControl\n",
    "import torchvision.transforms as T\n",
    "from safetensors import safe_open\n",
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "from utils.dataset import process_frames\n",
    "from utils.gaussian_2d import get_guassian_2d_rand_mask\n",
    "from utils.util import blip_cap\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "seed = 12345\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "cond_x = False\n",
    "\n",
    "image_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/inputs/House.jpg\"\n",
    "depth_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/inputs/House_depth.png\"\n",
    "if cond_x:\n",
    "    input_path = image_path\n",
    "else:\n",
    "    input_path = depth_path\n",
    "\n",
    "pipeline = StableDiffusionInpaintPipeline.from_pretrained(\"runwayml/stable-diffusion-inpainting\",\n",
    "                                                      safety_checker=None, \n",
    "                                                      torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "# depth_lora_path = \"/data/juicefs_sharing_data/11162591/code/lxr/svd-train/output_depth_lora/checkpoint-17000\"\n",
    "# pipeline.load_lora_weights(depth_lora_path, adapter_name=\"default\")\n",
    "\n",
    "checkpoint_dir = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/output_dir/output_mask_depth_lora/checkpoint-5000\"\n",
    "\n",
    "if not cond_x:\n",
    "    mask_depth_lora_path = os.path.join(checkpoint_dir, \"y_lora\")\n",
    "    pipeline.load_lora_weights(mask_depth_lora_path, adapter_name = \"mask_depth_lora\")\n",
    "\n",
    "# state_dict = {}\n",
    "# with safe_open(\"/data/juicefs_sharing_data/11162591/code/lxr/svd-train/output_depth_lora_rank64/checkpoint-2500/model.safetensors\", framework=\"pt\", device=0) as f:\n",
    "#     for k in f.keys():\n",
    "#         state_dict[k] = f.get_tensor(k)\n",
    "# pipeline.unet.load_state_dict(state_dict)\n",
    "\n",
    "pipeline = pipeline.to(\"cuda\").to(torch.float16)\n",
    "\n",
    "meta_data_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/readout_guidance/readout_training/data/metadata.jsonl\"\n",
    "import json\n",
    "with open(meta_data_path, \"r\") as f:\n",
    "    lines = list(f)\n",
    "    lines = [json.loads(line) for line in lines]\n",
    "\n",
    "\n",
    "use_noise_lora = False\n",
    "\n",
    "cond_id = 1428\n",
    "cond_image_name = lines[cond_id][\"original_image\"] if cond_x else lines[cond_id][\"file_name\"]\n",
    "prompt = lines[cond_id][\"text\"]\n",
    "\n",
    "depth_root = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/readout_guidance/readout_training/data/pseudo_labels/PascalVOC/depth\"\n",
    "depth_path = os.path.join(depth_root, cond_image_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119c6acd-3050-42e5-8a63-5ed0c498d3ff",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "if input_path is not None:\n",
    "    # depth_path = input_path\n",
    "    depth_image = Image.open(input_path)\n",
    "    # a = Image.open(input_path)\n",
    "    if depth_image.mode == \"I\":\n",
    "        depth_tensor = T.ToTensor()(depth_image)\n",
    "        depth_tensor = (depth_tensor - depth_tensor.min()) / (depth_tensor.max() - depth_tensor.min())\n",
    "        depth_image = T.ToPILImage()(depth_tensor)\n",
    "    depth_image = depth_image.convert(\"RGB\")\n",
    "    raw_image  = Image.open(image_path).convert(\"RGB\")\n",
    "    prompt = blip_cap([depth_image])[0]\n",
    "    # prompt = \"a house\"\n",
    "else:\n",
    "    depth_image = Image.open(depth_path).convert(\"RGB\")\n",
    "\n",
    "print(f\"Load cond image from {depth_path}\")\n",
    "\n",
    "# depth_image = T.Resize(512)(depth_image)\n",
    "# depth_image = T.CenterCrop(512)(depth_image)\n",
    "\n",
    "# depth_image = T.ToTensor()(depth_image).to(\"cuda\").to(torch.float16)\n",
    "# height, width = depth_image.shape[-2:]\n",
    "\n",
    "batch_size = 4\n",
    "# depth_images = torch.stack([depth_image] * batch_size)\n",
    "\n",
    "# depth_images = (depth_images - 0.5) / 0.5\n",
    "# depth_latents = pipeline.vae.encode(depth_images).latent_dist.sample()\n",
    "# depth_latents = depth_latents * pipeline.vae.config.scaling_factor\n",
    "# depth_latents = None\n",
    "print(prompt)\n",
    "\n",
    "# generator = torch.Generator(device=depth_latents.device)\n",
    "# generator = generator.manual_seed(123)\n",
    "\n",
    "# channel = 4\n",
    "\n",
    "\n",
    "# torch.random.manual_seed(1111)\n",
    "# latents = torch.randn(batch_size // 2, channel, height // 8, width // 8).to(\"cuda\").to(torch.float16)\n",
    "# latents = torch.cat([latents] * 2)\n",
    "\n",
    "# if use_noise_lora:\n",
    "#     torch.manual_seed(123)\n",
    "#     fix_noise = torch.randn(4, 512, 512)\n",
    "#     condition_noise = fix_noise[None,...,:depth_latents.shape[-2],:depth_latents.shape[-1]].expand_as(depth_latents)\n",
    "#     cond_mask = torch.zeros_like(condition_noise).to(torch.bool)\n",
    "#     cond_mask[..., depth_latents.shape[-2] // 2:,:depth_latents.shape[-1]] = 1\n",
    "#     # cond_mask = 1\n",
    "# else:\n",
    "# condition_noise = latents\n",
    "\n",
    "\n",
    "height, width = 512, 512\n",
    "init_image = depth_image\n",
    "init_image = process_frames([init_image], height, width)[0]\n",
    "# height, width = image.height, image.width\n",
    "# rand_mask = get_guassian_2d_rand_mask(grid_size, noise_patch_size * 8)\n",
    "# rand_mask = rand_mask[None,None,:height,:width].expand(batch_size, 1, height. width)\n",
    "# init_image = load_image(\"https://hf-mirror.com/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png\")\n",
    "height, width = init_image.height, init_image.width\n",
    "\n",
    "patch_size = 1\n",
    "\n",
    "rand_mask = get_guassian_2d_rand_mask((max(height, width) // 8 // patch_size) + 1, patch_size, thresh = 0)\n",
    "rand_mask = rand_mask[None,:height // 8,:width // 8].expand(3, height // 8, width // 8).to(torch.float32)\n",
    "mask_image = T.ToPILImage()(rand_mask)\n",
    "\n",
    "mask_image_resize = process_frames([mask_image], height, width)[0]\n",
    "# mask_image = load_image(\"https://hf-mirror.com/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png\")\n",
    "# generator = torch.Generator(\"cuda\").manual_seed(92)\n",
    "# prompt = \"concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k\"\n",
    "# prompt = args.validation_prompt\n",
    "\n",
    "# for _ in range(args.num_validation_images):\n",
    "images = pipeline(\n",
    "        [prompt] * batch_size,\n",
    "        image = [init_image] * batch_size,\n",
    "        mask_image = [mask_image] * batch_size,\n",
    "        height = height,\n",
    "        width = width,\n",
    "        # num_inference_steps=30, \n",
    "        # generator=generator\n",
    "    ).images\n",
    "image_grid = make_image_grid([init_image, mask_image_resize, *images], rows=1, cols=2 + len(images))\n",
    "\n",
    "expnum = 0\n",
    "save_path = os.path.join(checkpoint_dir, \"test_results\")\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "\n",
    "image_grid.save(os.path.join(save_path, f\"depth_test_{expnum}.png\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b358b4e-57df-4369-a4b1-1967af3d0f94",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1ea08a-5cb2-4be5-b805-82406086b4f4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from PIL import Image\n",
    "# from diffusers import AutoPipelineForText2Image\n",
    "# import torch\n",
    "# from patch import patch\n",
    "# import os\n",
    "# from diffusers import AutoencoderKL, DDPMScheduler, DiffusionPipeline, StableDiffusionPipeline, UNet2DConditionModel\n",
    "# from diffusers.loaders import UNet2DConditionLoadersMixin, PeftAdapterMixin, FromSingleFileMixin\n",
    "# from pipeline.pipeline_stable_diffusion_joint_control import StableDiffusionPipelineJointControl\n",
    "# import torchvision.transforms as T\n",
    "# from safetensors import safe_open\n",
    "# from utils.gaussian_2d import get_guassian_2d_rand_mask\n",
    "\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "from patch import patch\n",
    "# from diffusers import AutoPipelineForText2Image\n",
    "import torch\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, DiffusionPipeline, StableDiffusionPipeline, UNet2DConditionModel, DDIMScheduler\n",
    "from diffusers.loaders import UNet2DConditionLoadersMixin, PeftAdapterMixin, FromSingleFileMixin\n",
    "from pipeline.pipeline_stable_diffusion_joint_control import StableDiffusionPipelineJointControl\n",
    "import torchvision.transforms as T\n",
    "from safetensors import safe_open\n",
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "from utils.dataset import process_frames\n",
    "from utils.gaussian_2d import get_guassian_2d_rand_mask\n",
    "from utils.util import blip_cap\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8502457b-60f6-44ad-8bc5-86369cbf1e3a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_noise_lora = False\n",
    "\n",
    "model_id = \"runwayml/stable-diffusion-inpainting\"\n",
    "# model_id = \"/root/data/juicefs_sharing_data/11162591/code/models/runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "pipeline = StableDiffusionInpaintPipeline.from_pretrained(model_id,\n",
    "                                                      safety_checker=None, \n",
    "                                                      torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127af878-8e50-4843-a896-e5b3e7f990b3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline.scheduler, DDIMScheduler.from_config(pipeline2.scheduler.config), pipe.scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0334f9df-da20-483c-a919-a225820e8641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline2 = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\",\n",
    "                                                      safety_checker=None, \n",
    "                                                      torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9016e0-36fa-4bae-ad6f-eed27c0f3e28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "depth_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/inputs/House_midas_depth.png\"\n",
    "depth_image = Image.open(depth_path).convert(\"RGB\")\n",
    "depth_tensor = T.ToTensor()(depth_image).to(torch.float16).to(\"cuda\")\n",
    "depth_tensor = depth_tensor * 2 - 1\n",
    "depth_tensor = depth_tensor.unsqueeze(0)\n",
    "depth_latents = pipeline.vae.encode(depth_tensor).latent_dist.sample() * pipeline.vae.config.scaling_factor\n",
    "recon_depth_tensor = pipeline.vae.decode(\n",
    "    depth_latents  / pipeline.vae.config.scaling_factor, return_dict=False\n",
    ")[0] \n",
    "recon_depth_tensor = recon_depth_tensor.clamp(min = -1, max = 1)\n",
    "recon_depth_image = T.ToPILImage()((recon_depth_tensor[0] + 1.) / 2.)\n",
    "recon_depth_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92ece88-6603-4071-98ef-b7900920bc55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7ddf94-416b-4d10-891a-a16feeb1d341",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "patch.apply_patch(pipeline.unet)\n",
    "patch.initialize_joint_layers(pipeline.unet)\n",
    "\n",
    "# y_lora_path = \"/data/juicefs_sharing_data/11162591/code/lxr/svd-train/output_depth_lora/pytorch_lora_weights.safetensors\"\n",
    "# pipeline.load_lora_weights(y_lora_path, adapter_name=\"y_lora\")\n",
    "\n",
    "# load attention processors\n",
    "# output_path = \"output_lora_joint_depth_image_joint_randt/checkpoint-10500\"\n",
    "# output_path = \"output_lora_joint_depth_image_jointfix/checkpoint-5000\"\n",
    "\n",
    "# output_path = \"/data/juicefs_sharing_data/11162591/code/lxr/svd-train/output_lora_joint_depth_image_clean_cond\"\n",
    "# output_path = \"output_lora_joint_depth_image_clean_cond/checkpoint-5000\"\n",
    "output_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/output_dir/output_mask_depth_lora_joint\"\n",
    "y_lora_path = os.path.join(output_path, \"y_lora\")\n",
    "pipeline.load_lora_weights(y_lora_path, adapter_name=\"y_lora\")\n",
    "\n",
    "for lora_name in [\"xy_lora\", \"yx_lora\"]:\n",
    "    save_dir = os.path.join(output_path, f\"{lora_name}\")\n",
    "    # lora_state_dict, lora_network_alphas = StableDiffusionPipeline.lora_state_dict(save_dir)\n",
    "    # StableDiffusionPipeline.load_lora_into_unet(lora_state_dict, lora_network_alphas, unet = pipeline.unet, adapter_name = lora_name)\n",
    "    pipeline.load_lora_weights(save_dir, adapter_name=lora_name)\n",
    "\n",
    "# state_dict = torch.load(os.path.join(output_path, \"model.safetensors\"), map_location=\"cpu\")\n",
    "\n",
    "# rec_txt1 = open('output_lora_joint_depth_image_clean_cond1.txt', 'w')\n",
    "\n",
    "# for name, para in pipeline.unet.named_parameters():\n",
    "#     rec_txt1.write(f'{name}\\n')\n",
    "\n",
    "# rec_txt1.close()\n",
    "\n",
    "state_dict = {}\n",
    "model_path = os.path.join(output_path, \"model.safetensors\")\n",
    "if os.path.exists(model_path):\n",
    "    with safe_open(model_path, framework=\"pt\", device=0) as f:\n",
    "        for k in f.keys():\n",
    "            state_dict[k] = f.get_tensor(k)\n",
    "    pipeline.unet.load_state_dict(state_dict)\n",
    "else:\n",
    "    state_dict_dir = os.path.join(output_path, \"model.pth\")\n",
    "    state_dict = torch.load(state_dict_dir)\n",
    "    pipeline.unet.load_state_dict(state_dict, strict = False)\n",
    "\n",
    "\n",
    "# rec_txt1 = open('output_lora_joint_depth_image_clean_cond2.txt', 'w')\n",
    "\n",
    "# for name, para in pipeline.unet.named_parameters():\n",
    "#     rec_txt1.write(f'{name}\\n')\n",
    "\n",
    "# rec_txt1.close()\n",
    "\n",
    "\n",
    "\n",
    "active_adapters = [\"xy_lora\", \"yx_lora\", \"y_lora\"]\n",
    "\n",
    "pipeline.set_adapters(active_adapters)\n",
    "patch.hack_lora_forward(pipeline.unet)\n",
    "patch.set_patch_lora_mask(pipeline.unet, \"y_lora\", [0,1,0,1])\n",
    "patch.set_patch_lora_mask(pipeline.unet, \"yx_lora\", [0,1,0,1])\n",
    "patch.set_patch_lora_mask(pipeline.unet, \"xy_lora\", [1,0,1,0])\n",
    "\n",
    "pipeline = pipeline.to(\"cuda\").to(torch.float16)\n",
    "\n",
    "\n",
    "image_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/inputs/room.jpg\"\n",
    "depth_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/inputs/room_depth.jpg\"\n",
    "\n",
    "\n",
    "# depth_path = input_path\n",
    "raw_image = Image.open(image_path)\n",
    "depth_image = Image.open(depth_path)\n",
    "# a = Image.open(input_path)\n",
    "if depth_image.mode == \"I\":\n",
    "    depth_tensor = T.ToTensor()(depth_image)\n",
    "    depth_tensor = 1 - (depth_tensor - depth_tensor.min()) / (depth_tensor.max() - depth_tensor.min())\n",
    "    \n",
    "    depth_image = T.ToPILImage()(depth_tensor)\n",
    "\n",
    "depth_image = depth_image.convert(\"RGB\")\n",
    "\n",
    "prompt = blip_cap([raw_image])[0]\n",
    "\n",
    "print(f\"Load cond image from {image_path}, cond depth from {depth_path}\")\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "height, width = 512, 512\n",
    "patch_size = 1\n",
    "\n",
    "\n",
    "seed = 123\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "generator = torch.Generator(device=\"cuda\")\n",
    "generator = generator.manual_seed(seed)\n",
    "height, width = 512, 512\n",
    "init_images = [raw_image,  depth_image]\n",
    "init_images = process_frames(init_images, height, width)\n",
    "init_images = [init_images[0]] * (batch_size // 2) + [init_images[1]] * (batch_size // 2)\n",
    "# height, width = image.height, image.width\n",
    "# rand_mask = get_guassian_2d_rand_mask(grid_size, noise_patch_size * 8)\n",
    "# rand_mask = rand_mask[None,None,:height,:width].expand(batch_size, 1, height. width)\n",
    "# init_image = load_image(\"https://hf-mirror.com/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png\")\n",
    "height, width = init_images[0].height, init_images[0].width\n",
    "rand_mask = [get_guassian_2d_rand_mask((max(height, width) // 8 // patch_size) + 1, patch_size, thresh = 0) for i in range(batch_size)]\n",
    "rand_mask = torch.stack(rand_mask, dim = 0)\n",
    "rand_mask = rand_mask[:, None,:height // 8,:width // 8].expand(batch_size, 3, height // 8, width // 8).to(torch.float32)\n",
    "\n",
    "rand_mask[:,:,:,rand_mask.shape[-1] // 2:] = 0.0\n",
    "rand_mask[:,:,:,:rand_mask.shape[-1] // 2] = 1.0\n",
    "\n",
    "# rand_mask[:] = 0\n",
    "\n",
    "rand_mask[batch_size // 2:] = 1 - rand_mask[:batch_size // 2]\n",
    "mask_images = [T.ToPILImage()(mask) for mask in rand_mask]\n",
    "# mask_image = load_image(\"https://hf-mirror.com/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png\")\n",
    "# generator = torch.Generator(\"cuda\").manual_seed(92)\n",
    "# prompt = \"concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k\"\n",
    "\n",
    "\n",
    "images = pipeline(\n",
    "        [prompt] * batch_size,\n",
    "        image = init_images,\n",
    "        mask_image = mask_images,\n",
    "        height = height,\n",
    "        width = width,\n",
    "        # num_inference_steps=30, \n",
    "        generator=generator\n",
    "    ).images\n",
    "\n",
    "mask_images_upscale = process_frames(mask_images, height, width)\n",
    "\n",
    "images_with_mask = []\n",
    "for im, miu in zip(images, mask_images_upscale):\n",
    "    # Create a mask with transparency\n",
    "    inverted_mask = ImageOps.invert(miu.convert(\"L\"))\n",
    "    mask_with_alpha = Image.new('RGBA', inverted_mask.size)\n",
    "    mask_with_alpha.paste((255, 0, 0, 128), (0, 0), inverted_mask)  # Red color with 50% opacity\n",
    "\n",
    "    # Composite the overlay with the original image\n",
    "    image_with_mask = Image.alpha_composite(im.convert('RGBA'), mask_with_alpha)\n",
    "    images_with_mask.append(image_with_mask)\n",
    "\n",
    "image_grid = make_image_grid([*init_images, *mask_images_upscale, *images, *images_with_mask], rows=4, cols=batch_size)\n",
    "\n",
    "exp_name = os.path.basename(image_path).split(\".\")[0]\n",
    "save_path = os.path.join(output_path, \"test_results\")\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "\n",
    "image_grid.save(os.path.join(save_path, f\"{exp_name}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e1cf21-d708-47e7-ac59-d16ff3f330cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "depth_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/inputs/room_depth.jpg\"\n",
    "depth_image = Image.open(depth_path)\n",
    "depth_image = depth_image.convert(\"RGB\")\n",
    "depth_tensor = T.ToTensor()(depth_image)\n",
    "depth_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9283d88e-1c5d-43aa-acf8-1ed3e1e7b6e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "depth_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/inputs/House_depth.png\"\n",
    "depth_image = Image.open(depth_path)\n",
    "# a = Image.open(input_path)\n",
    "if depth_image.mode == \"I\":\n",
    "    depth_tensor = T.ToTensor()(depth_image)\n",
    "    depth_tensor = 1 - (depth_tensor - depth_tensor.min()) / (depth_tensor.max() - depth_tensor.min())\n",
    "    \n",
    "    depth_image = T.ToPILImage()(depth_tensor)\n",
    "output_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/inputs/House_depth.png\"\n",
    "depth_image = depth_image.convert(\"RGB\")\n",
    "depth_image.save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77cb8c6-5217-473e-981a-dce0d8957a58",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "gray_depth_root = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/PascalVOC/VOC2012/depth_anything_v2_gray/\"\n",
    "color_depth_root = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/PascalVOC/VOC2012/depth_anything_v2_color/\"\n",
    "image_root = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/PascalVOC/VOC2012/JPEGImages/\"\n",
    "depth_paths = glob.glob(os.path.join(gray_depth_root, \"*.jpg\"))\n",
    "\n",
    "depth_arr = np.array(depth_paths)\n",
    "rand_num = 5\n",
    "rand_depths = np.random.choice(depth_arr, size = rand_num, replace=False)\n",
    "\n",
    "output_root = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/inputs\"\n",
    "for path in rand_depths:\n",
    "    gray_depth_image = Image.open(path)\n",
    "    raw_image = Image.open(os.path.join(image_root, os.path.basename(path)))\n",
    "    color_depth_image = Image.open(os.path.join(color_depth_root, os.path.basename(path)))\n",
    "    gray_depth_image.save(os.path.join(output_root, \"gray_depth\" + os.path.basename(path)))\n",
    "    color_depth_image.save(os.path.join(output_root, \"color_depth\" + os.path.basename(path)))\n",
    "    raw_image.save(os.path.join(output_root, \"image\" + os.path.basename(path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15965b7-633e-4394-805d-e61f716fca65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "save_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/output_dir/output_mask_depth_lora/model.pth\"\n",
    "#save_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/output_dir/output_mask_depth_lora_joint_freezey/model.pth\"\n",
    "state_dict = torch.load(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7487925-16ff-4a06-8683-caf77cb97daf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state_dict = pipeline.unet.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49d10e7-d4f8-40e0-bf66-06e74d85d1d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_params = 0\n",
    "for param_tensor in state_dict.values():\n",
    "    total_params += param_tensor.numel()\n",
    "total_params / 1000 / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a662299d-31c7-41ba-9f2f-f10f1dfba57b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# def dilate(im_tensor, kernel_size = 3):\n",
    "#     dtype = im_tensor.dtype\n",
    "#     kernel = np.ones([kernel_size, kernel_size])\n",
    "#     kernel_tensor = torch.Tensor(np.expand_dims(np.expand_dims(kernel, 0), 0))\n",
    "#     result = torch.clamp(torch.nn.functional.conv2d(im_tensor, kernel_tensor, padding=(kernel_size // 2, kernel_size // 2)), 0, 1).int()\n",
    "#     return result.to(dtype)\n",
    "\n",
    "# def blur(im_tensor, kernel_size = 3):\n",
    "#     kernel = np.ones([kernel_size, kernel_size]) / (kernel_size ** 2)\n",
    "#     kernel_tensor = torch.Tensor(np.expand_dims(np.expand_dims(kernel, 0), 0))\n",
    "#     result = torch.clamp(torch.nn.functional.conv2d(im_tensor, kernel_tensor, padding=(kernel_size // 2, kernel_size // 2)), 0, 1)\n",
    "#     return result\n",
    "\n",
    "# def rand_mask_test(batch_size, grid_size, mask_size, thresh = None, noise_patch_size = 1, smooth = True, blur_kernel = 5, dilate_kernel = 7, dtype = torch.float32):\n",
    "#     # rand_grid_size = torch.randint(grid_size_range[0], grid_size_range[1], (bsz,))\n",
    "#     grid_sizes = [grid_size] * batch_size\n",
    "#     rand_masks = [get_guassian_2d_rand_mask(gs, noise_patch_size, thresh = thresh).view(1,1,grid_size, grid_size).to(dtype) for gs in grid_sizes]\n",
    "\n",
    "#     # rand_masks = [F.interpolate(rand_mask, size = mask_size, mode=\"bilinear\") for rand_mask in rand_masks]\n",
    "#     # kernel_sizes = [1,3,5,7,9,11,13,15,17]\n",
    "#     # rand_masks[0] = blur(rand_masks[0].view(1,1,*rand_masks[0].shape), 5)\n",
    "#     # rand_masks[0] = rand_masks[0].view(1,1,*rand_masks[0].shape)\n",
    "#     # rand_masks = [dilate(rand_masks[0], kernel_size) for kernel_size in kernel_sizes]\n",
    "#     rand_masks = torch.cat(rand_masks, dim = 0)\n",
    "#     if smooth:\n",
    "#         rand_masks = dilate(blur(rand_masks, blur_kernel), dilate_kernel)\n",
    "#         one_minus_rand_masks = 1 - rand_masks\n",
    "#         rand_tensor = (torch.rand(batch_size) < 0.5).view(-1, 1, 1, 1)\n",
    "#         rand_masks = torch.where(rand_tensor, rand_masks, one_minus_rand_masks) \n",
    "#         # rand_masks = blur(dilate(rand_masks, dilate_kernel), blur_kernel).int().to(torch.float32)\n",
    "        \n",
    "    \n",
    "\n",
    "#     # rand_masks = rand_masks.expand(bsz, 1, *mask_size)\n",
    "#     return rand_masks\n",
    "\n",
    "from utils.gaussian_2d import get_rand_masks, set_smooth_kernel\n",
    "set_smooth_kernel(blur_ks = 5, dilate_ks = 5)\n",
    "\n",
    "rand_masks = get_rand_masks(9, 64, (64, 64))\n",
    "rand_masks = [T.ToPILImage()(rand_mask) for rand_mask in rand_masks]\n",
    "rand_mask_grid = make_image_grid(rand_masks, rows = 3, cols = 3)\n",
    "# print(rand_grid_size)\n",
    "rand_mask_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc5399b-e9c2-4156-8e38-e3178942acc9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bf039f-b0f0-4115-9725-ee03cb24ff69",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HF_HOME\"] = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/.cache/huggingface\"\n",
    "\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# load pipe\n",
    "pipe = pipeline(task=\"depth-estimation\", model=\"depth-anything/Depth-Anything-V2-Large-hf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9fa1cd-e22c-4ea6-8fb6-ca8568bed2fd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# load image\n",
    "# url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "image = Image.open(\"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/inputs/House.jpg\")\n",
    "\n",
    "# inference\n",
    "res = pipe(image)\n",
    "image.height, image.width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eacede-206a-419d-8c25-6773cbc326fe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "res[\"predicted_depth\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26b9086-7afc-4d29-811f-c419f6669a1a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "import numpy as np\n",
    "depth_tensor = res[\"predicted_depth\"]\n",
    "# depth_tensor = (depth_tensor - depth_tensor.min()) / (depth_tensor.max() - depth_tensor.min()) * 255.0\n",
    "\n",
    "depth_tensor = torch.nn.functional.interpolate(\n",
    "    depth_tensor.unsqueeze(1),\n",
    "    size=image.size[::-1],\n",
    "    mode=\"bilinear\",\n",
    "    align_corners=False,\n",
    ")[0]\n",
    "\n",
    "depth_tensor = depth_tensor  / depth_tensor.max() * 255.0\n",
    "\n",
    "depth_tensor = depth_tensor.to(torch.uint8)\n",
    "\n",
    "cmap = matplotlib.colormaps.get_cmap('Spectral_r')\n",
    "colored_depth = (cmap(depth_tensor[0])[:, :, :3] * 255).astype(np.uint8)\n",
    "\n",
    "gray_depth = T.ToPILImage()(depth_tensor)\n",
    "\n",
    "color_depth = Image.fromarray(colored_depth)\n",
    "depth_tensor.shape, colored_depth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dab2e6-e10d-4d7e-964a-35b0bed941b7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "(cmap(depth_tensor[0]) * 255).astype(np.uint8)\n",
    "#https://stackoverflow.com/questions/45177154/how-to-decode-color-mapping-in-matplotlibs-colormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c32d15-39a0-43f5-aed4-ccea12bd2dcc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "image.size[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2daf1b5-2f07-4387-9cd8-1b8c0950dc27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms.v2 as transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "\n",
    "x_image = Image.open(requests.get(url, stream=True).raw)\n",
    "y_image = Image.open(requests.get(url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b273bcb-1ff5-4c6f-84ce-6a79e5f24898",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_image_trans, y_image_trans = train_transforms(x_image, y_image)\n",
    "print([x_image_trans.min(), x_image_trans.max()], [y_image_trans.min(), y_image_trans.max()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7520d8-fb42-4b11-bb66-cf862032c5a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_image_trans.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7254b6-fd93-46ef-95f2-d9e8b0502e54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ToDoList = {\n",
    "    \"Image fusion\",\n",
    "    \"Ablations: no encoder joint cross attention, postconv to postscale, 64 or 32\"\n",
    "    \"Camera Move\",\n",
    "    \"Marigold dataset\",\n",
    "    \"caption (https://github.com/pharmapsychotic/clip-interrogator)\",\n",
    "    \"Video frame joint generation\",\n",
    "    \"how to map pixel to point clouds\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f878dbb1-cc53-45fd-987d-71b264b9f9c7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms.v2 as transforms\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "\n",
    "x_image = Image.open(requests.get(url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c24d7a2-8274-45b8-b16c-6805310387f5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def random_train_transforms(hw_logratio_range = [np.log(9 / 16), np.log(16 / 9)]):\n",
    "import random \n",
    "import numpy as np\n",
    "hw_logratio_range = [np.log(0.66666666667), np.log(1.5)]\n",
    "total_pixels = 512 * 512\n",
    "hw_logratio = random.uniform(*hw_logratio_range)\n",
    "# hw_logratio = np.log(1.5)\n",
    "hw_ratio = np.e ** (hw_logratio)\n",
    "# hw_ratio =  0.5\n",
    "width = int(np.sqrt(total_pixels / hw_ratio) / 8 + 0.5) * 8\n",
    "height = int(total_pixels // width / 8 + 0.5) * 8\n",
    "# random_sizes = [(512, 512), (576, 448), (576, 384), (448, 576), (384, 576), (640, 448), (448, 640)]\n",
    "# height, width = random.choice(random_sizes)\n",
    "\n",
    "\n",
    "hw_ratio = height / width\n",
    "\n",
    "\n",
    "print(height, width, hw_ratio, np.sqrt(width * height))\n",
    "\n",
    "train_transforms = transforms.Compose(\n",
    "    [   \n",
    "        transforms.RandomResizedCrop(size = (height, width), scale = (0.5, 1.0), ratio = (1 / hw_ratio, 1 / hw_ratio))\n",
    "        # transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        # transforms.CenterCrop(args.resolution) if args.center_crop else transforms.RandomCrop(args.resolution),\n",
    "        # transforms.RandomHorizontalFlip() if args.random_flip else transforms.Lambda(lambda x: x),\n",
    "        # # transforms.ToTensor(),\n",
    "        # transforms.ToImage(),\n",
    "        # transforms.ToDtype(torch.float32, scale=True),\n",
    "        # transforms.Normalize([0.5], [0.5]),\n",
    "        # transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "train_transforms(x_image)\n",
    "    # return train_transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2a4a39-418d-4cd9-a121-e6952c572c15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9f25ab-16c5-4956-a12c-a6ae90650ea3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30038bf0-f7c1-4be4-8cc6-1d810ab656ac",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HF_HOME\"] = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/.cache/huggingface\"\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, DDIMScheduler\n",
    "import torch\n",
    "from utils.util import load_scheduler\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\"lllyasviel/control_v11p_sd15_seg\", torch_dtype=torch.float16, use_safetensors=True)\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16, use_safetensors=True\n",
    ")\n",
    "\n",
    "# pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0e34d8-17b7-43e3-a3b6-d850d6f4c731",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "load_scheduler(pipe, \"ead\")\n",
    "pipe.scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccdee62-1397-4bf6-bdab-10330cf68ca2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from utils.dataset import process_frames\n",
    "image = Image.open(\"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/inputs/seg-room-gt.webp\")\n",
    "prompt = \"best quality, extremely detailed, there is a living room with a couch, table, chairs and a television\"\n",
    "image = process_frames([image], h = 512, w = 512)[0]\n",
    "output = pipe(\n",
    "    prompt, image=image, guidance_scale=7.5, num_inference_steps=50, height = 512, width = 512, negative_prompt=\"longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality\"\n",
    ").images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee84d2d6-ab33-4085-867b-d007aa75a354",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# image = Image.open(\"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/inputs/seg-table-gt.webp\")\n",
    "# process_frames([image], h = 512, w = 512)[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49c1347-fd21-4073-bd33-7a8e49b2c62b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.gaussian_2d import get_rand_masks, set_smooth_kernel\n",
    "import torchvision.transforms as T\n",
    "height = width = 512\n",
    "grid_size = max(height, width)\n",
    "rand_masks = get_rand_masks(4, grid_size, noise_patch_size = 1, thresh = 0)\n",
    "rand_masks = rand_masks[...,:height, :width]\n",
    "mask_images = [T.ToPILImage()(mask) for mask in rand_masks]\n",
    "mask_images[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc41d811-f0e5-47fd-9062-f8a9dd73444a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "bsz = 32\n",
    "latents = torch.ones([32,4,64,64])\n",
    "zero_mask = torch.zeros_like(latents)\n",
    "one_mask = torch.ones_like(latents)\n",
    "condition = (torch.arange(bsz) < (bsz // 4)).view(-1, 1,1,1)\n",
    "rand_mask = torch.where(condition, zero_mask, one_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdb8ff7-326d-4da4-80d3-ae13bfaf0bb8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "rand_mask[8].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2e6269-b099-404c-90ba-284f0fd121d6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "image.height, image.width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328ac938-db88-4794-9979-f354047c85a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HF_HOME\"] = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/.cache/huggingface\"\n",
    "from transformers import OneFormerProcessor, OneFormerModel\n",
    "from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "# load OneFormer fine-tuned on ADE20k for universal segmentation\n",
    "processor = OneFormerProcessor.from_pretrained(\"shi-labs/oneformer_ade20k_swin_tiny\")\n",
    "model = OneFormerForUniversalSegmentation.from_pretrained(\"shi-labs/oneformer_ade20k_swin_tiny\")\n",
    "\n",
    "url = (\n",
    "    \"https://hf-mirror.com/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg\"\n",
    ")\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# Semantic Segmentation\n",
    "inputs = processor(image, [\"semantic\"], return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "# model predicts class_queries_logits of shape `(batch_size, num_queries)`\n",
    "# and masks_queries_logits of shape `(batch_size, num_queries, height, width)`\n",
    "class_queries_logits = outputs.class_queries_logits\n",
    "masks_queries_logits = outputs.masks_queries_logits\n",
    "\n",
    "# you can pass them to processor for semantic postprocessing\n",
    "predicted_semantic_map = processor.post_process_semantic_segmentation(\n",
    "    outputs, target_sizes=[image.size[::-1]]\n",
    ")[0]\n",
    "print(f\"ðŸ‘‰ Semantic Predictions Shape: {list(predicted_semantic_map.shape)}\")\n",
    "\n",
    "# Instance Segmentation\n",
    "inputs = processor(image, [\"instance\"], return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "# model predicts class_queries_logits of shape `(batch_size, num_queries)`\n",
    "# and masks_queries_logits of shape `(batch_size, num_queries, height, width)`\n",
    "class_queries_logits = outputs.class_queries_logits\n",
    "masks_queries_logits = outputs.masks_queries_logits\n",
    "\n",
    "# you can pass them to processor for instance postprocessing\n",
    "predicted_instance_map = processor.post_process_instance_segmentation(\n",
    "    outputs, target_sizes=[image.size[::-1]]\n",
    ")[0][\"segmentation\"]\n",
    "print(f\"ðŸ‘‰ Instance Predictions Shape: {list(predicted_instance_map.shape)}\")\n",
    "\n",
    "# Panoptic Segmentation\n",
    "inputs = processor(image, [\"panoptic\"], return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "# model predicts class_queries_logits of shape `(batch_size, num_queries)`\n",
    "# and masks_queries_logits of shape `(batch_size, num_queries, height, width)`\n",
    "class_queries_logits = outputs.class_queries_logits\n",
    "masks_queries_logits = outputs.masks_queries_logits\n",
    "\n",
    "# you can pass them to processor for panoptic postprocessing\n",
    "predicted_panoptic_map = processor.post_process_panoptic_segmentation(\n",
    "    outputs, target_sizes=[image.size[::-1]]\n",
    ")[0][\"segmentation\"]\n",
    "f\"ðŸ‘‰ Panoptic Predictions Shape: {list(predicted_panoptic_map.shape)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554ccbea-7d3f-4109-aeb0-740d93fc9a39",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicted_panoptic_map\n",
    "visualizer = Visualizer(image, metadata=self.metadata, instance_mode=ColorMode.IMAGE)\n",
    "    predictions = self.predictor(image, task)\n",
    "    panoptic_seg, segments_info = predictions[\"panoptic_seg\"]\n",
    "    vis_output['panoptic_inference'] = visualizer.draw_panoptic_seg_predictions(\n",
    "    panoptic_seg.to(self.cpu_device), segments_info, alpha=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d593e5be-09a2-4c48-8382-a804fb877480",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from detectron2 import data\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HF_HOME\"] = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/.cache/huggingface\"\n",
    "sys.path.append(\"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/ControlNet-v1-1-nightly\")\n",
    "from annotator.oneformer import OneformerCOCODetector, OneformerADE20kDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b1f7f4-a984-4995-88bc-ca3ada2e7ab5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "url = (\n",
    "    \"https://hf-mirror.com/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg\"\n",
    ")\n",
    "image = Image.open(requests.get(url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121e8e99-f35b-4230-8976-85796caafb3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_arr2 = np.stack([image_arr] * 2, axis = 0)\n",
    "output = preprocessor(image_arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e464d70a-ca0c-4ea4-a026-693bb09c25f4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "image_arr = np.array(image)\n",
    "preprocessor = OneformerADE20kDetector()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89cfe37-8d0a-494e-985c-f287f1df15da",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "Image.fromarray(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f80cb8-48f5-40af-b839-e2481559292c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFilter\n",
    "\n",
    "def create_feathered_circle(image_size, circle_radius, feather_radius):\n",
    "    # Create a new image with a white background\n",
    "    image = Image.new('L', image_size, 255)\n",
    "    \n",
    "    # Create a drawing context\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    # Draw a solid black circle in the middle\n",
    "    center = (image_size[0] // 2, image_size[1] // 2)\n",
    "    draw.ellipse((center[0] - circle_radius, center[1] - circle_radius, \n",
    "                  center[0] + circle_radius, center[1] + circle_radius), fill=0)\n",
    "    \n",
    "    # Apply a Gaussian blur to create the feathering effect\n",
    "    image = image.filter(ImageFilter.GaussianBlur(feather_radius))\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Parameters\n",
    "image_size = (512, 512)  # Size of the image\n",
    "circle_radius = 200      # Radius of the black circle\n",
    "feather_radius = 25      # Radius for the feather effect\n",
    "\n",
    "# Create the feathered circle image\n",
    "feathered_circle_image = create_feathered_circle(image_size, circle_radius, feather_radius)\n",
    "\n",
    "# Save the image\n",
    "# feathered_circle_image.save('feathered_circle.png')\n",
    "output_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/inputs/feathered_circle_mask_large.jpg\"\n",
    "feathered_circle_image.save(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1252113f-4e86-4f86-a873-3958ec0fd851",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "a = np.zeros([512,512])\n",
    "b = np.linspace(0, 1, 512).reshape(1, 512)\n",
    "a += b\n",
    "print(a.shape)\n",
    "a = (a * 255.).astype(np.uint8)\n",
    "\n",
    "mask = Image.fromarray(a)\n",
    "mask.save(\"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/inputs/feathered_horizontal_mask.jpg\")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b877d02-38cc-4b47-94d0-a31def1e9ab1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionControlNetInpaintPipeline, ControlNetModel, UniPCMultistepScheduler, StableDiffusionControlNetPipeline\n",
    "import torch\n",
    "# controlnet = ControlNetModel.from_pretrained(\"lllyasviel/control_v11p_sd15_inpaint\", torch_dtype=torch.float16, use_safetensors=True)\n",
    "controlnet = ControlNetModel.from_pretrained(\"lllyasviel/control_v11f1p_sd15_depth\", torch_dtype=torch.float16, use_safetensors=True)\n",
    "# pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(\n",
    "#     \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16, use_safetensors=True\n",
    "# )\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16, use_safetensors=True\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ba4c30-43e7-415c-98d1-b6da89011c48",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "from diffusers.utils import make_image_grid\n",
    "image_raw = Image.open(\"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/PascalVOC/VOC2012/depth_anything_v2_color/2007_003267.jpg\")\n",
    "image = T.ToTensor()(image_raw).to(torch.float16).to(\"cuda\")\n",
    "image = image * 2 - 1\n",
    "image_latents = pipe.vae.encode(image.unsqueeze(0)).latent_dist.sample()\n",
    "image = pipe.vae.decode(image_latents).sample[0]\n",
    "image = ((image + 1) / 2).clip(min = -1, max = 1)\n",
    "image = T.ToPILImage()(image)\n",
    "make_image_grid([image, image_raw], rows = 1, cols = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69a4a48-db6e-4496-abc2-e0519339631b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from diffusers.utils import load_image, make_image_grid\n",
    "\n",
    "init_image = load_image(\n",
    "    # \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/inputs/football.jpg\"\n",
    "    # \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/inputs/flower.jpg\"\n",
    "    \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/inputs/House.jpg\"\n",
    ")\n",
    "init_image = init_image.resize((512, 512))\n",
    "\n",
    "mask_image = load_image(\n",
    "    # \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/inputs/football-mask-invert.webp\"\n",
    "    \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/inputs/flower-mask-invert.webp\"\n",
    ")\n",
    "mask_image = mask_image.resize((512, 512))\n",
    "make_image_grid([init_image, mask_image], rows=1, cols=2)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def make_inpaint_condition(image, image_mask):\n",
    "    image = np.array(image.convert(\"RGB\")).astype(np.float32) / 255.0\n",
    "    image_mask = np.array(image_mask.convert(\"L\")).astype(np.float32) / 255.0\n",
    "\n",
    "    assert image.shape[0:1] == image_mask.shape[0:1]\n",
    "    image[image_mask > 0.5] = -1.0  # set as masked pixel\n",
    "    image = np.expand_dims(image, 0).transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image)\n",
    "    return image\n",
    "\n",
    "# control_image = make_inpaint_condition(init_image, mask_image)\n",
    "control_image = load_image(\n",
    "    # \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/inputs/floor-depth-gray.png\"\n",
    "    # \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/inputs/table-depth-gray.png\"\n",
    "    \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/inputs/House-depth-color.webp\"\n",
    ")\n",
    "\n",
    "from utils.dataset import process_frames\n",
    "\n",
    "control_image = process_frames([control_image], h = 512, w = 512)[0]\n",
    "\n",
    "from utils.util import load_scheduler\n",
    "# pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "load_scheduler(pipe, \"ead\")\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "output = pipe(\n",
    "    # \"best quality, extremely detailed, a football lying on the floor of a living room\",\n",
    "    # \"best quality, extremely detailed, a football lying on the table in a room\",\n",
    "    \"best quality, extremely detailed, there is a digital image of a man holding a tennis racket\",\n",
    "    negative_prompt=\"longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality\",\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=7.5,\n",
    "    strength=1.0,\n",
    "    # eta=1.0,\n",
    "    image=init_image,\n",
    "    # mask_image=mask_image,\n",
    "    control_image=control_image,\n",
    "    height = 512,\n",
    "    width = 512\n",
    ").images[0]\n",
    "make_image_grid([init_image, mask_image, control_image, output], rows=2, cols=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9482fd-cdd9-4825-bd38-3b0995163d0e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "\n",
    "from utils.dataset import TrackDataset\n",
    "from utils import train_helpers\n",
    "import numpy as np\n",
    "import pdb\n",
    "from PIL import Image\n",
    "\n",
    "config_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/train_models/train_configs/track_dataset.yaml\"\n",
    "\n",
    "config = OmegaConf.load(config_path)\n",
    "config = OmegaConf.to_container(config, resolve=True)\n",
    "# train_dataset, train_dataloader = get_correspondence_loader(config, config[\"train_file\"], True)\n",
    "\n",
    "dataset = TrackDataset(**config)\n",
    "it = iter(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e01923-9ddc-4747-9890-ba7599ba3590",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"goat\" in st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc9a5fc-cee4-412a-b169-4a535745fe5f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "st = set([d[\"source\"].split(\"/\")[0] for d in dataset.data])\n",
    "print(len(st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3394044-937a-443a-b634-6dcbf9b92b45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d008817-fc11-4340-bb92-2492dae30d20",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "a[\"source_frame\"][0].save(\"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/inputs/example_inputs/riding-source.png\")\n",
    "a[\"target_frame\"][0].save(\"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/inputs/example_inputs/riding-target.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f99d9d-ef62-4b90-b3d5-82e7021bdbc1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6f7125-c1c8-438c-afdc-2954ae511d67",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "a[\"source_frame\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced9d83e-1b18-4964-b6dd-c05884a4eafe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "a[\"target_frame\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96beee6-f509-4994-ac6d-c0140e7b419a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "exp_name = \"man-crocodile\"\n",
    "source_image_path = f\"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/readout_guidance/data/drag/real/{exp_name}/source.png\"\n",
    "source_image_pil = Image.open(source_image_path)\n",
    "\n",
    "import numpy as np\n",
    "tracks = np.load(f\"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/readout_guidance/data/drag/real/{exp_name}/tracks_mouth-close.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb63777-9853-4607-ae47-77a4f2876d54",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def draw_tracks_target(target_image_pil, tracks, line_width = 5, arrow_width = 5, alpha = 0.5, color = (0, 255, 0)):\n",
    "    source_points = tracks[0,...]\n",
    "    target_points = tracks[1,...]\n",
    "    target_image_pil_track = target_image_pil \n",
    "\n",
    "    for src_point, tgt_point in zip(source_points, target_points):\n",
    "        # target_image_pil_points = add_transparent_point(target_image_pil_points.convert(\"RGBA\"), tgt_point, 5, (0, 255,0), 127) \n",
    "        # print(src_point, tgt_point)\n",
    "        print(tgt_point, src_point)\n",
    "        target_image_pil_track = draw_arrow(target_image_pil_track.convert(\"RGBA\"), \n",
    "            tgt_point, src_point, line_width, color, int(alpha * 255), arrow_width)\n",
    "\n",
    "    return target_image_pil_track\n",
    "draw_tracks_target(source_image_pil, tracks[0,::-1]).save(f\"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/inputs/example_inputs/drag_{exp_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c873c865-50d8-403e-a964-4f7e1fbfbcd0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "from importlib import reload\n",
    "import random\n",
    "reload(train_helpers)\n",
    "# from utils import train_helpers\n",
    "\n",
    "idx = random.randint(0, len(dataset) - 1)\n",
    "\n",
    "item = dataset.data[idx]\n",
    "print(item)\n",
    "# pdb.set_trace()\n",
    "source_image_pil = Image.open(os.path.join(dataset.bucket_root, dataset.image_root, item[\"source\"]))\n",
    "target_image_pil = Image.open(os.path.join(dataset.bucket_root, dataset.image_root, item[\"target\"]))\n",
    "source = np.array(source_image_pil)\n",
    "target = np.array(target_image_pil)\n",
    "\n",
    "i, j = train_helpers.get_frame_idx(item[\"source\"]), train_helpers.get_frame_idx(item[\"target\"])\n",
    "\n",
    "# Load point tracks\n",
    "video_name = item[\"source\"].split(\"/\")[0]\n",
    "tracks, visibles = train_helpers.open_points(i, j, video_name, dataset.max_frame_idx, dataset.bucket_root, dataset.points_root)\n",
    "mask = train_helpers.open_mask(i, video_name, dataset.bucket_root, dataset.mask_root)\n",
    "if random.random() < 0.5:\n",
    "    mask = None\n",
    "height, width, _ = source.shape\n",
    "print(tracks.shape)\n",
    "tracks_full = train_helpers.filter_tracks(tracks, width, height, mask, dataset.num_points)\n",
    "print(tracks_full.shape)\n",
    "# visibles = visibles[:, valid_idxs]\n",
    "# print(tracks_full.shape)\n",
    "\n",
    "import random\n",
    "def sample_track(tracks, visibles = None, mask = None, num_samples = 5, reg = 0.1):\n",
    "    # print(tracks)\n",
    "    track_norm = np.linalg.norm(tracks[0] - tracks[1], axis=-1)\n",
    "    if visibles is not None:\n",
    "        track_norm[~visibles] = 0\n",
    "    track_norm = (track_norm - track_norm.mean()) / (track_norm.std() + 1e-6)\n",
    "    # print(sorted(track_norm, reverse =True)[:10])\n",
    "    # track_prob = track_norm / sum(track_norm)\n",
    "    # print(track_norm)\n",
    "    track_norm -= np.max(track_norm)\n",
    "    # print(track_norm)\n",
    "    track_prob = (np.exp(track_norm)) / sum(np.exp(track_norm))\n",
    "    # print(track_prob)\n",
    "    track_prob = (track_prob + (reg / len(track_prob))) / (1 + reg)\n",
    "    # print(track_prob)\n",
    "    \n",
    "    # print(sorted(track_prob, reverse =True)[:10])\n",
    "    # track_prob[:] = 0\n",
    "    # track_prob[0] = 1\n",
    "    # print(track_prob.shape)\n",
    "    num_samples = min(tracks.shape[1], num_samples)\n",
    "    sample_idx = np.random.choice(tracks.shape[1], num_samples, replace=False, p = track_prob)\n",
    "    return sample_idx\n",
    "# visible_and = np.logical_and(visibles[0], visibles[1])\n",
    "sample_idx = [sample_track(tracks_full, num_samples=random.randint(1, 10)) for i in range(1)]\n",
    "sample_idx = np.concatenate(sample_idx)\n",
    "tracks = tracks_full[:, sample_idx, :]\n",
    "\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFilter\n",
    "import torch\n",
    "def draw_arrow(image, start_point, end_point, arrow_width, arrow_color, arrow_alpha, arrowhead_size = 5):\n",
    "    \n",
    "\n",
    "    # Create a temporary image for drawing the arrow with transparency\n",
    "    temp_image = Image.new('RGBA', image.size, (255, 255, 255, 0))\n",
    "    temp_image = temp_image.resize(image.size)\n",
    "    temp_draw = ImageDraw.Draw(temp_image)\n",
    "\n",
    "    # Draw the arrow body\n",
    "    temp_draw.line([*start_point, *end_point], fill=arrow_color + (arrow_alpha,), width=arrow_width)\n",
    "\n",
    "    # Calculate the arrowhead\n",
    "    # arrowhead_size = 15  # Size of the arrowhead\n",
    "    direction = ((end_point[0] - start_point[0]), (end_point[1] - start_point[1]))  # Direction of the arrow\n",
    "    direction_magnitude = (direction[0]**2 + direction[1]**2)**0.5  # Magnitude of the direction vector\n",
    "    normalized_direction = (direction[0]/direction_magnitude, direction[1]/direction_magnitude)  # Normalized direction vector\n",
    "\n",
    "    # Calculate the arrowhead points\n",
    "    left_arrowhead_point = (\n",
    "        end_point[0] - arrowhead_size * normalized_direction[1],\n",
    "        end_point[1] + arrowhead_size * normalized_direction[0]\n",
    "    )\n",
    "    right_arrowhead_point = (\n",
    "        end_point[0] + arrowhead_size * normalized_direction[1],\n",
    "        end_point[1] - arrowhead_size * normalized_direction[0]\n",
    "    )\n",
    "\n",
    "    top_arrowhead_point = (\n",
    "        end_point[0] + arrowhead_size * normalized_direction[0],\n",
    "        end_point[1] + arrowhead_size * normalized_direction[1]\n",
    "    )\n",
    "\n",
    "    # Draw the arrowhead\n",
    "    temp_draw.polygon([*top_arrowhead_point, *left_arrowhead_point, *right_arrowhead_point], fill=arrow_color + (arrow_alpha,))\n",
    "\n",
    "    # Combine the temporary image with the original image\n",
    "    combined = Image.alpha_composite(image, temp_image)\n",
    "\n",
    "    return combined\n",
    "\n",
    "def add_transparent_point(image, position, point_radius, color, alpha):\n",
    "    # Create a transparent layer\n",
    "    overlay = Image.new('RGBA', image.size, (255, 255, 255, 0))\n",
    "    draw = ImageDraw.Draw(overlay)\n",
    "    \n",
    "    # Draw a semi-transparent point\n",
    "    draw.ellipse((position[0] - point_radius, position[1] - point_radius, \n",
    "                  position[0] + point_radius, position[1] + point_radius), fill=(color[0], color[1], color[2], alpha))\n",
    "    \n",
    "    # Composite the original image with the transparent overlay\n",
    "    image = Image.alpha_composite(image, overlay)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def draw_tracks(source_image_pil, target_image_pil, tracks):\n",
    "    source_points = torch.tensor(tracks[0,...])\n",
    "    target_points = torch.tensor(tracks[1,...])\n",
    "    source_image_pil_points = source_image_pil\n",
    "    target_image_pil_points = target_image_pil\n",
    "    for src_point, tgt_point in zip(source_points, target_points):\n",
    "        # source_image_pil_points = add_transparent_point(source_image_pil_points.convert(\"RGBA\"), src_point, 5, (0, 255,0), 127) \n",
    "        # print(src_point, tgt_point)\n",
    "        source_image_pil_points = draw_arrow(source_image_pil_points.convert(\"RGBA\"), src_point.numpy(), tgt_point.numpy(), 5, (0, 255,0), 255) \n",
    "\n",
    "    for src_point, tgt_point in zip(source_points, target_points):\n",
    "        # target_image_pil_points = add_transparent_point(target_image_pil_points.convert(\"RGBA\"), tgt_point, 5, (0, 255,0), 127) \n",
    "        # print(src_point, tgt_point)\n",
    "        target_image_pil_points = draw_arrow(target_image_pil_points.convert(\"RGBA\"), tgt_point.numpy(), src_point.numpy(), 5, (0, 255,0), 127) \n",
    "\n",
    "def draw_tracks_target(target_image_pil, tracks, line_width = 5, arrow_width = 5, alpha = 0.5, color = (0, 255, 0)):\n",
    "    source_points = tracks[0,...]\n",
    "    target_points = tracks[1,...]\n",
    "    target_image_pil_track = target_image_pil \n",
    "\n",
    "    for src_point, tgt_point in zip(source_points, target_points):\n",
    "        # target_image_pil_points = add_transparent_point(target_image_pil_points.convert(\"RGBA\"), tgt_point, 5, (0, 255,0), 127) \n",
    "        # print(src_point, tgt_point)\n",
    "        target_image_pil_track = draw_arrow(target_image_pil.convert(\"RGBA\"), \n",
    "            tgt_point.numpy(), src_point.numpy(), line_width, color, int(alpha * 255), arrow_width)\n",
    "\n",
    "    return target_image_pil_track\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "width1, height1 = source_image_pil_points.size\n",
    "width2, height2 = target_image_pil_points.size\n",
    "\n",
    "# Determine dimensions of the combined image\n",
    "total_width = width1 + width2 * 2\n",
    "max_height = max(height1, height2)\n",
    "\n",
    "# Create a new blank image with the determined dimensions\n",
    "combined_image = Image.new('RGB', (total_width, max_height))\n",
    "\n",
    "# Paste the first image at (0, 0)\n",
    "combined_image.paste(source_image_pil_points, (0, 0))\n",
    "\n",
    "\n",
    "# Paste the second image at (width1, 0)\n",
    "combined_image.paste(target_image_pil_points, (width1, 0))\n",
    "if mask is not None:\n",
    "    print(\"merge mask\")\n",
    "    combined_image.paste(mask, (width1 + width2, 0))\n",
    "\n",
    "# combined_image.paste(mask, (width1 + width2, 0))\n",
    "combined_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf847a8-0473-4be6-b4be-a09a6427d1cd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms.v2 as T\n",
    "import matplotlib\n",
    "import torch\n",
    "import numpy as np\n",
    "image_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/inputs/example_inputs/snowman.png\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "to_tensor = T.Compose([\n",
    "    T.ToImage(),\n",
    "    T.ToDtype(torch.float32, scale=True),\n",
    "])\n",
    "\n",
    "depth_tensor = to_tensor(image)\n",
    "cmap = matplotlib.colormaps.get_cmap('Spectral_r')\n",
    "\n",
    "depth_tensor_flatten = depth_tensor.flatten()\n",
    "depth_tensor_flatten = depth_tensor_flatten.sort().values\n",
    "dlen = depth_tensor_flatten.shape[0]\n",
    "print((depth_tensor_flatten == 1).sum() / dlen)\n",
    "\n",
    "dmin, dmax = depth_tensor_flatten[int(dlen * 0.02)], depth_tensor_flatten[int(dlen * 0.98) - 1]\n",
    "print(dmin, dmax)\n",
    "print(depth_tensor_flatten.min(), depth_tensor_flatten.max())\n",
    "\n",
    "\n",
    "\n",
    "depth_tensor = (depth_tensor - dmin) / (dmax - dmin) * 255.0\n",
    "\n",
    "# depth_tensor[depth_tensor == 255] = 0\n",
    "depth_tensor = depth_tensor.clamp(min = 0, max = 255)\n",
    "print(depth_tensor.min(), depth_tensor.max())\n",
    "depth_tensor = depth_tensor.to(torch.uint8)\n",
    "color_depth_array = (cmap(depth_tensor[0])[:, :, :3] * 255).astype(np.uint8)\n",
    "color_depth = Image.fromarray(color_depth_array)\n",
    "color_depth.save(image_path.replace(\".png\", \"_color.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7885b7-6ec0-4ce6-b479-67f26cd4a20c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "input_root = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/annotate/annotations/\"\n",
    "input_file = \"PascalVOC_val.json\"\n",
    "output_root = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/annotate/\"\n",
    "\n",
    "with open(os.path.join(output_root, input_file), \"r\") as f:\n",
    "    a = json.load(f)\n",
    "\n",
    "# b = [aa['source'] for aa in a] \n",
    "# with open(os.path.join(output_root, input_file), \"w\") as f:\n",
    "#     json.dump(b, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdc5cf7-23fd-4779-b510-98f3dd8751f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e75fa2-b542-4718-b3bd-4ae61693a426",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c9dbc8-d2db-4258-9be1-9acb28bf5946",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/PascalVOC/VOC2012/depth/val_dataset.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1697343f-e5d8-4d52-ae8a-06f6d4d7a2f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "next(iter(dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac7004d-a7f8-4bd0-9b50-b4b514dc5f51",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "dataset_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/PascalVOC/VOC2012/depth/val_dataset.json\"\n",
    "val_dataset = json.load(open(dataset_path))\n",
    "\n",
    "\n",
    "\n",
    "image_paths = [line['original_image'] for line in val_dataset]\n",
    "\n",
    "real_images = [Image.open(path).convert(\"RGB\") for path in image_paths]\n",
    "\n",
    "from torchvision.transforms import functional as F\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from utils.dataset import process_frames\n",
    "\n",
    "height, width = 512, 512\n",
    "\n",
    "ToTensor = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToImage(),\n",
    "        transforms.ToDtype(torch.float32, scale=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = process_frames([image], height, width,  verbose = False, div = 8, rand_crop=False)[0]\n",
    "    image = ToTensor(image)\n",
    "    # return F.center_crop(image, (256, 256))\n",
    "    return image\n",
    "\n",
    "\n",
    "processed_real_images = torch.stack([preprocess_image(image) for image in real_images], dim = 0)\n",
    "print(processed_real_images.shape)\n",
    "# torch.Size([10, 3, 256, 256])\n",
    "\n",
    "from glob import glob\n",
    "generate_image_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/output_final/output_depth_gray_controlnet/eval\"\n",
    "generate_image_paths = [os.path.join(generate_image_path, line['file_name']) for line in val_dataset]\n",
    "\n",
    "fake_images = [Image.open(path).convert(\"RGB\") for path in generate_image_paths]\n",
    "processed_fake_images = torch.stack([preprocess_image(image) for image in fake_images], dim = 0)\n",
    "print(processed_real_images.shape)\n",
    "\n",
    "processed_fake_images.min(), processed_fake_images.max()\n",
    "\n",
    "\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "\n",
    "fid = FrechetInceptionDistance(normalize=True)\n",
    "fid.update(processed_real_images, real=True)\n",
    "fid.update(processed_fake_images, real=False)\n",
    "\n",
    "print(f\"FID: {float(fid.compute())}\")\n",
    "\n",
    "prompt_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/annotate/PascalVOC_prompt.json\"\n",
    "with open(prompt_path, \"r\") as f:\n",
    "    prompt_dict = json.load(f)\n",
    "# image_paths = [line['original_image'] for line in val_dataset]\n",
    "prompts = [prompt_dict[line['file_name']][0] for line in val_dataset]\n",
    "\n",
    "from torchmetrics.functional.multimodal import clip_score\n",
    "from functools import partial\n",
    "\n",
    "clip_score_fn = partial(clip_score, model_name_or_path=\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "def calculate_clip_score(images, prompts):\n",
    "    images_int = (images * 255).to(torch.uint8)\n",
    "    clip_score = clip_score_fn(images_int, prompts).detach()\n",
    "    return round(float(clip_score), 4)\n",
    "\n",
    "\n",
    "sd_clip_score = calculate_clip_score(processed_fake_images, prompts)\n",
    "print(f\"CLIP score: {sd_clip_score}\")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "import clip\n",
    "from PIL import Image, ImageFile\n",
    "import pdb\n",
    "\n",
    "\n",
    "class MLP(pl.LightningModule):\n",
    "    def __init__(self, input_size, xcol='emb', ycol='avg_rating'):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.xcol = xcol\n",
    "        self.ycol = ycol\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.input_size, 1024),\n",
    "            #nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 128),\n",
    "            #nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            #nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            nn.Linear(64, 16),\n",
    "            #nn.ReLU(),\n",
    "\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch[self.xcol]\n",
    "        y = batch[self.ycol].reshape(-1, 1)\n",
    "        x_hat = self.layers(x)\n",
    "        loss = F.mse_loss(x_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch[self.xcol]\n",
    "        y = batch[self.ycol].reshape(-1, 1)\n",
    "        x_hat = self.layers(x)\n",
    "        loss = F.mse_loss(x_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "def normalized(a, axis=-1, order=2):\n",
    "    import numpy as np  # pylint: disable=import-outside-toplevel\n",
    "\n",
    "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
    "    l2[l2 == 0] = 1\n",
    "    return a / np.expand_dims(l2, axis)\n",
    "\n",
    "def load_models():\n",
    "    model = MLP(768)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    s = torch.load(\"sac+logos+ava1-l14-linearMSE.pth\", map_location=device)\n",
    "\n",
    "    model.load_state_dict(s)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    model2, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "\n",
    "    model_dict = {}\n",
    "    model_dict['classifier'] = model\n",
    "    model_dict['clip_model'] = model2\n",
    "    model_dict['clip_preprocess'] = preprocess\n",
    "    model_dict['device'] = device\n",
    "\n",
    "    return model_dict\n",
    "\n",
    "def predict(images, batch_size = 100):\n",
    "    image_inputs = [model_dict['clip_preprocess'](image).unsqueeze(0).to(model_dict['device']) for image in images]\n",
    "    image_inputs = torch.cat(image_inputs)\n",
    "    with torch.no_grad():\n",
    "        all_embs = []\n",
    "        # pdb.set_trace()\n",
    "        for image_input_batch in image_inputs.split(batch_size):\n",
    "            image_features = model_dict['clip_model'].encode_image(image_input_batch)\n",
    "            if model_dict['device'] == 'cuda':\n",
    "                im_emb_arr = normalized(image_features.detach().cpu().numpy())\n",
    "                im_emb = torch.from_numpy(im_emb_arr).to(model_dict['device']).type(torch.cuda.FloatTensor)\n",
    "            else:\n",
    "                im_emb_arr = normalized(image_features.detach().numpy())\n",
    "                im_emb = torch.from_numpy(im_emb_arr).to(model_dict['device']).type(torch.FloatTensor)\n",
    "            all_embs.append(im_emb)\n",
    "        all_embs = torch.cat(all_embs)\n",
    "        prediction = model_dict['classifier'](all_embs)\n",
    "    score = prediction.mean().item()\n",
    "\n",
    "    return {'aesthetic score': score}\n",
    "\n",
    "model_dict = load_models()\n",
    "\n",
    "score = predict(fake_images)\n",
    "\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f937a08-eac8-48aa-a1a7-df34035df21c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e74f73b-5aa5-4690-bd2a-51f900c111ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57904e90-f074-43af-ae2a-a9e0c2407461",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f404efef-80f7-48fc-96e5-a3c4c78e49ec",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create CelebA id preserving dataset\n",
    "\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import pandas\n",
    "import json\n",
    "\n",
    "file_mapping_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/CelebAMask-HQ/CelebA-HQ-to-CelebA-mapping.txt\"\n",
    "id_mapping_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/CelebA/Anno/identity_CelebA.txt\"\n",
    "\n",
    "file_list = pandas.read_csv(file_mapping_path, delim_whitespace = True).values.tolist()\n",
    "\n",
    "id_mapping = pandas.read_csv(id_mapping_path, delim_whitespace = True, header=None)\n",
    "id_mapping = id_mapping.values.tolist()\n",
    "id_mapping = {img: idx for img, idx in id_mapping}\n",
    "\n",
    "prompt_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/CelebAMask-HQ/CelebAMask-HQ-prompts.json\"\n",
    "\n",
    "with open(prompt_path, \"r\") as f:\n",
    "    prompt_dict = json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "celebahq_root = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/CelebAMask-HQ/CelebA-HQ-img\"\n",
    "celeba_root = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/CelebA/img_align_celeba\"\n",
    "celeba_wild_root = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/CelebA/img_celeba\"\n",
    "\n",
    "celeba_wild_dataset_list = []\n",
    "celeba_hq_dataset_list = []\n",
    "\n",
    "id_dict = defaultdict(list)\n",
    "\n",
    "def process_blip_prompt(prompt):\n",
    "    return prompt.replace(\"arafed\", \"\").replace(\"araffe\", \"\").replace(\"there is\", \"\").replace(\"  \", \" \").strip(\" \")\n",
    "\n",
    "for img_idx, orig_idx, orig_file in file_list:\n",
    "    person_id = id_mapping[orig_file]\n",
    "    raw_image_prompt, align_hq_image_prompt = prompt_dict[str(img_idx)]\n",
    "\n",
    "    aligned_hq_image_file = os.path.join(celebahq_root, f\"{img_idx}.jpg\")\n",
    "    # orig_file = os.path.join(celeba_root, orig_file)\n",
    "    raw_image_file = os.path.join(celeba_wild_root, orig_file)\n",
    "\n",
    "    \n",
    "\n",
    "    new_align_hq_image_prompt = align_hq_image_prompt.replace(\"a close up of \", \"\")\n",
    "\n",
    "    new_raw_image_prompt, new_align_hq_image_prompt = process_blip_prompt(raw_image_prompt), process_blip_prompt(new_align_hq_image_prompt)\n",
    "\n",
    "    new_align_hq_image_prompt = \"a close up of \" + new_align_hq_image_prompt\n",
    "\n",
    "    # data_item = {\"image_hq\": current_file, \"image\": orig_file, \"image_raw\": orig_wild_file}\n",
    "    id_dict[person_id].append([raw_image_file, aligned_hq_image_file, new_raw_image_prompt, new_align_hq_image_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c1f026b-f432-41b3-96e1-83242741c36f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T12:42:55.912132Z",
     "iopub.status.busy": "2024-09-01T12:42:55.887914Z",
     "iopub.status.idle": "2024-09-01T12:42:57.586703Z",
     "shell.execute_reply": "2024-09-01T12:42:57.585851Z",
     "shell.execute_reply.started": "2024-09-01T12:42:55.912075Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pair_len_names = [str(i) for i in range(0, 22, 1)]\n",
    "pair_len_names[-1] = \"> 20\"\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].pie(lens, labels = pair_len_names)\n",
    "plt.show()\n",
    "# ax[0].pie(pair_lens_summarized, labels = pair_len_names)\n",
    "# ax[1].pie(lens_summarized, labels = pair_len_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf72e603-0f33-46f2-9d67-72a4abe5dd19",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pair_lens_summarized = [0 for i in range(0, max(pair_lens), 50)]\n",
    "lens_summarized = [0 for i in range(0, max(pair_lens), 50)]\n",
    "\n",
    "pair_len_names = [\"<\" + str(i + 50) for i in range(0, max(pair_lens), 50)]\n",
    "\n",
    "for pl in pair_lens:\n",
    "    pair_lens_summarized[pl // 50] += pl\n",
    "    lens_summarized[pl // 50] += 1\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "\n",
    "ax[0].pie(pair_lens_summarized, labels = pair_len_names)\n",
    "ax[1].pie(lens_summarized, labels = pair_len_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1898289c-cd48-456b-ab17-ef696a288a5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T12:59:28.870143Z",
     "iopub.status.busy": "2024-09-01T12:59:28.869720Z",
     "iopub.status.idle": "2024-09-01T12:59:31.195856Z",
     "shell.execute_reply": "2024-09-01T12:59:31.189520Z",
     "shell.execute_reply.started": "2024-09-01T12:59:28.870105Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import random\n",
    "\n",
    "\n",
    "def make_paired_id_dataset(id_dict, per_id_pair_num = 50):\n",
    "\n",
    "    all_pairs = []\n",
    "    for person_id, image_list in id_dict.items():\n",
    "        if len(image_list) > 1:\n",
    "            # print(image_list)\n",
    "            possible_pairs = list(combinations(image_list, 2))\n",
    "            pair_list = []\n",
    "\n",
    "            while len(pair_list) < per_id_pair_num:\n",
    "                pair_list += random.sample(possible_pairs, len(possible_pairs))\n",
    "            \n",
    "            pair_list = pair_list[:per_id_pair_num]\n",
    "            \n",
    "            # print(possible_pairs)\n",
    "            # return possible_pairs\n",
    "            # pair_lens.append(len(possible_pairs))\n",
    "            all_pairs += pair_list\n",
    "    \n",
    "    return all_pairs\n",
    "            # print(possible_pairs)\n",
    "            # return possible_pairs\n",
    "\n",
    "# for per_id_pair_num in range(10, 101, 10):\n",
    "all_pairs = make_paired_id_dataset(id_dict, per_id_pair_num = 50)\n",
    "    # len(make_paired_id_dataset(id_dict))\n",
    "    # print(per_id_pair_num, len(all_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3da7e31f-e5d8-48df-ae30-406981c8c4e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T13:03:04.954551Z",
     "iopub.status.busy": "2024-09-01T13:03:04.954118Z",
     "iopub.status.idle": "2024-09-01T13:03:06.321926Z",
     "shell.execute_reply": "2024-09-01T13:03:06.320497Z",
     "shell.execute_reply.started": "2024-09-01T13:03:04.954509Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "celeba_wild_dataset_list = [{\"image0\": raw_image0, \"image1\": raw_image1, \"person_id\": person_id, \"text0\": text0, \"text1\": text1}\n",
    "    for (raw_image0, _, text0, _), (raw_image1, _, text1, _) in all_pairs\n",
    "]\n",
    "celeba_hq_dataset_list = [{\"image0\": aligned_hq_image0, \"image1\": aligned_hq_image1, \"person_id\": person_id, \"text0\": text0, \"text1\": text1}\n",
    "    for (_, aligned_hq_image0, _, text0), (_, aligned_hq_image1, _, text1) in all_pairs\n",
    "]\n",
    "\n",
    "celeba_dataset_list = [{\"image0_wild\": raw_image0, \"image1_wild\": raw_image1, \"person_id\": person_id, \"text0_wild\": text0_wild, \"text1_wild\": text1_wild,\n",
    "\"image0_align\": aligned_hq_image0, \"image1_align\": aligned_hq_image1,\"text0_align\": text0_align, \"text1_align\": text1_align}\n",
    "    for (raw_image0, aligned_hq_image0, text0_wild, text0_align), (raw_image1, aligned_hq_image1, text1_wild, text1_align) in all_pairs\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "506b96f6-9138-4ea9-b2bf-a33b49f55d9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T13:06:26.328553Z",
     "iopub.status.busy": "2024-09-01T13:06:26.328133Z",
     "iopub.status.idle": "2024-09-01T13:06:26.334952Z",
     "shell.execute_reply": "2024-09-01T13:06:26.334176Z",
     "shell.execute_reply.started": "2024-09-01T13:06:26.328516Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225800"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(celeba_dataset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d9fc134-321b-422e-9595-55425f312f61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T13:07:22.401697Z",
     "iopub.status.busy": "2024-09-01T13:07:22.400650Z",
     "iopub.status.idle": "2024-09-01T13:08:17.989331Z",
     "shell.execute_reply": "2024-09-01T13:08:17.988250Z",
     "shell.execute_reply.started": "2024-09-01T13:07:22.401650Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "celeba_dataset_list# celeba_hq_dataset_list\n",
    "celeba_hq_dataset_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/CelebAMask-HQ/dataset_celebahq_idpairs_200k.json\"\n",
    "celeba_wild_dataset_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/CelebA/dataset_celeba_wild_idpairs_200k.json\"\n",
    "celeba_full_dataset_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/CelebA/dataset_celeba_idpairs_200k.json\"\n",
    "with open(celeba_hq_dataset_path, \"w\") as f:\n",
    "    json.dump(celeba_hq_dataset_list, f)\n",
    "with open(celeba_wild_dataset_path, \"w\") as f:\n",
    "    json.dump(celeba_wild_dataset_list, f)\n",
    "with open(celeba_full_dataset_path, \"w\") as f:\n",
    "    json.dump(celeba_dataset_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8744be58-07ad-45dd-ae53-57dabbc883e4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from utils.util import blip_cap\n",
    "import pandas\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pdb\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "file_mapping_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/CelebAMask-HQ/CelebA-HQ-to-CelebA-mapping.txt\"\n",
    "file_list = pandas.read_csv(file_mapping_path, delim_whitespace = True).values.tolist()\n",
    "\n",
    "\n",
    "blip_processor = BlipProcessor.from_pretrained(\n",
    "        \"Salesforce/blip-image-captioning-large\", local_files_only = True)\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\n",
    "        \"Salesforce/blip-image-captioning-large\", torch_dtype=torch.float16, local_files_only = True).to(\"cuda\")\n",
    "# blip_processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "# blip_model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "#     \"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16\n",
    "# ).to(\"cuda\")\n",
    "\n",
    "\n",
    "celebahq_root = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/CelebAMask-HQ/CelebA-HQ-img\"\n",
    "celeba_root = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/CelebA/img_align_celeba\"\n",
    "celeba_wild_root = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/CelebA/img_celeba\"\n",
    "\n",
    "dataset_list = []\n",
    "\n",
    "\n",
    "raw_image_list = []\n",
    "aligned_hq_image_list = []\n",
    "image_key_list = []\n",
    "\n",
    "infer_batch_size = 16\n",
    "\n",
    "image_prompts_dict = dict()\n",
    "\n",
    "\n",
    "\n",
    "for i, (img_idx, orig_idx, orig_file) in enumerate(tqdm(file_list)):\n",
    "\n",
    "    current_file = os.path.join(celebahq_root, f\"{img_idx}.jpg\")\n",
    "    orig_wild_file = os.path.join(celeba_wild_root, orig_file)\n",
    "    raw_image_list.append(Image.open(orig_wild_file))\n",
    "    aligned_hq_image_list.append(Image.open(current_file))\n",
    "\n",
    "    image_key_list.append(img_idx)\n",
    "\n",
    "    if (i + 1) % infer_batch_size == 0 or i == len(file_list):\n",
    "        raw_image_prompts = blip_cap(raw_image_list, blip_processor, blip_model)\n",
    "        aligned_hq_image_prompts = blip_cap(aligned_hq_image_list, blip_processor, blip_model)\n",
    "\n",
    "        image_prompts_dict.update(\n",
    "            {\n",
    "                image_idx: [raw_image_prompt, aligned_hq_image_prompt]\n",
    "                for image_idx, raw_image_prompt, aligned_hq_image_prompt in \n",
    "                zip(image_key_list, raw_image_prompts, aligned_hq_image_prompts)\n",
    "            }\n",
    "        )\n",
    "        raw_image_list = []\n",
    "        aligned_hq_image_list = []\n",
    "        image_key_list = []\n",
    "\n",
    "\n",
    "with open(\"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/CelebAMask-HQ/CelebAMask-HQ-prompts.json\", \"w\") as f:\n",
    "    json.dump(image_prompts_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbc21c4-a2af-4db4-bdb4-ccde1778ecb4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/PascalVOC/VOC2012/depth_anything_v2_gray/val_dataset.json\", \"r\") as f:\n",
    "    a = json.load(f)\n",
    "# a[\"2007_000346.jpg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23233d6-7d63-4f7b-9e5c-555eff608c64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with open(\"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/annotate/PascalVOC_val.json\", \"r\") as f:\n",
    "    b = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54163e35-c23b-4383-ab38-609920d59bf6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/output_final/output_depth_lora_joint_gray_rank64_nta/eval_yad09\"\n",
    "cnt = 0\n",
    "for item in b:\n",
    "    \n",
    "    ip = os.path.join(path, item)\n",
    "    if not os.path.exists(ip):\n",
    "        cnt += 1\n",
    "print(cnt)\n",
    "        # print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a4731d-a634-48f6-9e30-e4bcb5385827",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/output_final/output_depth_lora_joint_gray_rank64_nta/eval_yad09\"\n",
    "for item in a:\n",
    "    ip = os.path.join(path, item[\"file_name\"])\n",
    "    if not os.path.exists(ip):\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f879dcbf-fa29-4b1c-a56f-2edb9020eb9c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.dataset import TrackDataset\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "config_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/svd-train/train_models/train_configs/track_dataset_val.yaml\"\n",
    "config = OmegaConf.load(config_path)\n",
    "config = OmegaConf.to_container(config, resolve=True)\n",
    "# train_dataset, train_dataloader = get_correspondence_loader(config, config[\"train_file\"], True)\n",
    "\n",
    "dataset = TrackDataset(**config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be915de3-2260-4ddc-a6e2-df8a9bc1dbc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T16:25:37.850503Z",
     "iopub.status.busy": "2024-09-01T16:25:37.850273Z",
     "iopub.status.idle": "2024-09-01T16:25:42.322323Z",
     "shell.execute_reply": "2024-09-01T16:25:42.320477Z",
     "shell.execute_reply.started": "2024-09-01T16:25:37.850479Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/jointdiff/lib/python3.9/site-packages/torchvision/datasets/video_utils.py:219: UserWarning: There aren't enough frames in the current video to get a clip for the given clip length and frames between clips. The video (and potentially others) will be skipped.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length 363225\n",
      "sample size 512\n"
     ]
    }
   ],
   "source": [
    "from utils.dataset import PandaN\r\n",
    "\r\n",
    "dataset_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/Panda70M/validation_videos.json\"\r\n",
    "# config = OmegaConf.load(config_path)\r\n",
    "# config = OmegaConf.to_container(config, resolve=True)\r\n",
    "# train_dataset, train_dataloader = get_correspondence_loader(config, config[\"train_file\"], True)\r\n",
    "\r\n",
    "dataset = PandaN(dataset_file = dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83395d1-c965-4658-b659-dbf1e89c49de",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d759a98-0bf7-4060-81bc-b4ab2381d54c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T15:57:19.501157Z",
     "iopub.status.busy": "2024-09-01T15:57:19.500754Z",
     "iopub.status.idle": "2024-09-01T15:57:19.910547Z",
     "shell.execute_reply": "2024-09-01T15:57:19.909521Z",
     "shell.execute_reply.started": "2024-09-01T15:57:19.501120Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\r\n",
    "import os\r\n",
    "dataset_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/Panda70M/validation\"\r\n",
    "\r\n",
    "video_files = glob.glob(os.path.join(dataset_path, \"**\", \"*.mp4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9429613b-ee07-4abb-a9cd-845a9a2c49ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-01T15:57:21.792541Z",
     "iopub.status.busy": "2024-09-01T15:57:21.792135Z",
     "iopub.status.idle": "2024-09-01T15:57:21.819225Z",
     "shell.execute_reply": "2024-09-01T15:57:21.818352Z",
     "shell.execute_reply.started": "2024-09-01T15:57:21.792503Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\r\n",
    "output_path = \"/home/bml/storage/mnt/v-95c5b44cfcff4e6c/org/data_lxr/data/Panda70M/validation_videos.json\"\r\n",
    "with open(output_path, \"w\") as f:\r\n",
    "    json.dump(video_files,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177d337d-b989-4bd9-90a9-480c9d268781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jointdiff)",
   "language": "python",
   "name": "jointdiff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
